{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhqHZ+lMz/qkz4xoTLG2lK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CoderRdm/Placement_Cell/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b9N6ADbxqGaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab requests pandas beautifulsoup4 selenium\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from google.colab import files\n",
        "import json\n",
        "from urllib.parse import quote\n",
        "\n",
        "leetcode_profiles = [\n",
        "    \"https://leetcode.com/Divyansh_Joshi_MNIT/\",\n",
        "    \"https://leetcode.com/Dhruv_parashar673/\",\n",
        "    \"https://leetcode.com/shivansh_codes/\",\n",
        "    \"https://leetcode.com/hardik7427/\",\n",
        "    \"https://leetcode.com/Nayu_1501/\",\n",
        "    \"https://leetcode.com/mayank_kumar123/\",\n",
        "    \"https://leetcode.com/tushardhakad355/\",\n",
        "    \"https://leetcode.com/rajatkhedar123/\",\n",
        "    \"https://leetcode.com/Dishank_Jha/\",\n",
        "    \"https://leetcode.com/Khushal_Saini/\",\n",
        "    \"https://leetcode.com/mauliksidana09/\",\n",
        "    \"https://leetcode.com/rudra_singh_07/\",\n",
        "    \"https://leetcode.com/govindsingh_777/\",\n",
        "    \"https://leetcode.com/Vinay-Gupta/\",\n",
        "    \"https://leetcode.com/Shivam_goyal_/\",\n",
        "    \"https://leetcode.com/padmesh_0150/\",\n",
        "    \"https://leetcode.com/Gaurav_1810/\",\n",
        "    \"https://leetcode.com/likhitd/\",\n",
        "    \"https://leetcode.com/Divyanshverma15/\",\n",
        "    \"https://leetcode.com/kushagras_94/\",\n",
        "    \"https://leetcode.com/harshit3458/\",\n",
        "    \"https://leetcode.com/RachitMittal1634/\",\n",
        "    \"https://leetcode.com/saurabh_32/\",\n",
        "    \"https://leetcode.com/tehseen1639/\",\n",
        "    \"https://leetcode.com/ruchika_yadav/\",\n",
        "    \"https://leetcode.com/yugsarda/\",\n",
        "    \"https://leetcode.com/Kartik_Mahnot/\",\n",
        "    \"https://leetcode.com/ViNiT-72/\",\n",
        "    \"https://leetcode.com/RK_Patel/\",\n",
        "    \"https://leetcode.com/beingKashvi/\",\n",
        "    \"https://leetcode.com/vikas_singh_856/\",\n",
        "    \"https://leetcode.com/user2633T/\",\n",
        "    \"https://leetcode.com/ShourayaKaushik/\",\n",
        "    \"https://leetcode.com/jatin-agrawal/\",\n",
        "    \"https://leetcode.com/Bot-Netizen-Programmers/\",\n",
        "    \"https://leetcode.com/bhargav-1673/\",\n",
        "    \"https://leetcode.com/Lakshit_Ramani/\",\n",
        "    \"https://leetcode.com/dhakad_09/\",\n",
        "    \"https://leetcode.com/NAregarded/\",\n",
        "    \"https://leetcode.com/akshaypal_bishnoi/\",\n",
        "    \"https://leetcode.com/godika_priya/\",\n",
        "    \"https://leetcode.com/Tush1586/\",\n",
        "    \"https://leetcode.com/Sarvesh25/\",\n",
        "    \"https://leetcode.com/user8201yw/\",\n",
        "    \"https://leetcode.com/Swayam_141/\",\n",
        "    \"https://leetcode.com/Kashishgarg_15/\",\n",
        "    \"https://leetcode.com/princimantri_2990/\",\n",
        "    \"https://leetcode.com/ankit4092004_Ankit___/\",\n",
        "    \"https://leetcode.com/pritamzzziscodingpranav_1686/\",\n",
        "    \"https://leetcode.com/Dhruvik_MYI/\",\n",
        "    \"https://leetcode.com/pankaj1213/\",\n",
        "    \"https://leetcode.com/kavya1502_/\",\n",
        "    \"https://leetcode.com/suhaani17/\",\n",
        "    \"https://leetcode.com/Dikshit_Rao/\",\n",
        "    \"https://leetcode.com/krishnasharma1234/\",\n",
        "    \"https://leetcode.com/KRITI_BHATNAGAR/\",\n",
        "    \"https://leetcode.com/Roshan_nama12/\",\n",
        "    \"https://leetcode.com/gautam_chauhan04/\",\n",
        "    \"https://leetcode.com/ayushkumar85371/\",\n",
        "    \"https://leetcode.com/Gulab_Rana/\",\n",
        "    \"https://leetcode.com/utkarshmodi10/\",\n",
        "    \"https://leetcode.com/Yash_07___077/\",\n",
        "    \"https://leetcode.com/MJ_Champ/\",\n",
        "    \"https://leetcode.com/AyushTak/\",\n",
        "    \"https://leetcode.com/SIY7gDUBSu/\"\n",
        "]\n",
        "\n",
        "class LeetCodeScraper:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        })\n",
        "\n",
        "    def extract_from_script_tags(self, soup, username):\n",
        "        \"\"\"Extract data from JavaScript variables in script tags\"\"\"\n",
        "        script_tags = soup.find_all('script')\n",
        "\n",
        "        for script in script_tags:\n",
        "            if script.string:\n",
        "                content = script.string\n",
        "\n",
        "                # Look for JSON data patterns\n",
        "                if 'submissionList' in content or 'contestBadge' in content:\n",
        "                    try:\n",
        "                        # Extract JSON-like data using regex\n",
        "                        json_match = re.search(r'(\\{.*\\})', content)\n",
        "                        if json_match:\n",
        "                            # This would need more specific parsing\n",
        "                            pass\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "        return {}\n",
        "\n",
        "    def scrape_with_graphql(self, username):\n",
        "        \"\"\"Try to use LeetCode's GraphQL API\"\"\"\n",
        "        try:\n",
        "            graphql_url = \"https://leetcode.com/graphql/\"\n",
        "\n",
        "            # Query for user profile\n",
        "            query = \"\"\"\n",
        "            query getUserProfile($username: String!) {\n",
        "                matchedUser(username: $username) {\n",
        "                    username\n",
        "                    submitStats {\n",
        "                        acSubmissionNum {\n",
        "                            difficulty\n",
        "                            count\n",
        "                        }\n",
        "                    }\n",
        "                    profile {\n",
        "                        ranking\n",
        "                        userAvatar\n",
        "                        realName\n",
        "                    }\n",
        "                }\n",
        "                userContestRanking(username: $username) {\n",
        "                    attendedContestsCount\n",
        "                    rating\n",
        "                    globalRanking\n",
        "                    totalParticipants\n",
        "                }\n",
        "            }\n",
        "            \"\"\"\n",
        "\n",
        "            payload = {\n",
        "                'query': query,\n",
        "                'variables': {'username': username}\n",
        "            }\n",
        "\n",
        "            response = self.session.post(graphql_url, json=payload, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "\n",
        "                if 'data' in data and data['data'].get('matchedUser'):\n",
        "                    user_data = data['data']['matchedUser']\n",
        "                    contest_data = data['data'].get('userContestRanking', {})\n",
        "\n",
        "                    # Parse submission stats\n",
        "                    easy = medium = hard = 0\n",
        "                    if user_data.get('submitStats', {}).get('acSubmissionNum'):\n",
        "                        for stat in user_data['submitStats']['acSubmissionNum']:\n",
        "                            if stat['difficulty'] == 'Easy':\n",
        "                                easy = stat['count']\n",
        "                            elif stat['difficulty'] == 'Medium':\n",
        "                                medium = stat['count']\n",
        "                            elif stat['difficulty'] == 'Hard':\n",
        "                                hard = stat['count']\n",
        "\n",
        "                    return {\n",
        "                        \"Username\": username,\n",
        "                        \"Total Problems Solved\": easy + medium + hard,\n",
        "                        \"Easy\": easy,\n",
        "                        \"Medium\": medium,\n",
        "                        \"Hard\": hard,\n",
        "                        \"Contest Rating\": contest_data.get('rating', 0) if contest_data else 0,\n",
        "                        \"Contests Attended\": contest_data.get('attendedContestsCount', 0) if contest_data else 0,\n",
        "                        \"Global Ranking\": contest_data.get('globalRanking', 'N/A') if contest_data else 'N/A',\n",
        "                        \"Status\": \"Success (GraphQL)\"\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            print(f\"GraphQL failed for {username}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def scrape_direct_html(self, username, url):\n",
        "        \"\"\"Direct HTML scraping with improved parsing\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                # Initialize default values\n",
        "                result = {\n",
        "                    \"Username\": username,\n",
        "                    \"Total Problems Solved\": 0,\n",
        "                    \"Easy\": 0,\n",
        "                    \"Medium\": 0,\n",
        "                    \"Hard\": 0,\n",
        "                    \"Contest Rating\": 0,\n",
        "                    \"Contests Attended\": 0,\n",
        "                    \"Global Ranking\": 'N/A',\n",
        "                    \"Status\": \"HTML Scraping\"\n",
        "                }\n",
        "\n",
        "                # Look for problem stats in various possible locations\n",
        "                # Method 1: Look for text patterns\n",
        "                text_content = soup.get_text()\n",
        "\n",
        "                # Extract numbers followed by \"Solved\" or similar patterns\n",
        "                solved_pattern = re.search(r'(\\d+)\\s*(?:Problems?\\s*)?Solved', text_content, re.IGNORECASE)\n",
        "                if solved_pattern:\n",
        "                    result[\"Total Problems Solved\"] = int(solved_pattern.group(1))\n",
        "\n",
        "                # Look for difficulty breakdowns\n",
        "                easy_pattern = re.search(r'Easy\\s*[:\\-]?\\s*(\\d+)', text_content, re.IGNORECASE)\n",
        "                if easy_pattern:\n",
        "                    result[\"Easy\"] = int(easy_pattern.group(1))\n",
        "\n",
        "                medium_pattern = re.search(r'Medium\\s*[:\\-]?\\s*(\\d+)', text_content, re.IGNORECASE)\n",
        "                if medium_pattern:\n",
        "                    result[\"Medium\"] = int(medium_pattern.group(1))\n",
        "\n",
        "                hard_pattern = re.search(r'Hard\\s*[:\\-]?\\s*(\\d+)', text_content, re.IGNORECASE)\n",
        "                if hard_pattern:\n",
        "                    result[\"Hard\"] = int(hard_pattern.group(1))\n",
        "\n",
        "                # Look for contest rating\n",
        "                rating_pattern = re.search(r'Rating\\s*[:\\-]?\\s*(\\d+)', text_content, re.IGNORECASE)\n",
        "                if rating_pattern:\n",
        "                    result[\"Contest Rating\"] = int(rating_pattern.group(1))\n",
        "\n",
        "                # Look for contests attended\n",
        "                contests_pattern = re.search(r'Attended\\s*[:\\-]?\\s*(\\d+)', text_content, re.IGNORECASE)\n",
        "                if contests_pattern:\n",
        "                    result[\"Contests Attended\"] = int(contests_pattern.group(1))\n",
        "\n",
        "                # Method 2: Look in script tags for JSON data\n",
        "                script_data = self.extract_from_script_tags(soup, username)\n",
        "                if script_data:\n",
        "                    result.update(script_data)\n",
        "\n",
        "                return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"HTML scraping failed for {username}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def try_multiple_apis(self, username):\n",
        "        \"\"\"Try multiple third-party APIs\"\"\"\n",
        "        apis = [\n",
        "            f\"https://leetcode-stats-api.herokuapp.com/{username}\",\n",
        "            f\"https://alfa-leetcode-api.onrender.com/{username}/solved\",\n",
        "            f\"https://leetcode-api-faisalshohag.vercel.app/{username}\",\n",
        "            f\"https://leetcodeapi-v1.vercel.app/{username}\"\n",
        "        ]\n",
        "\n",
        "        for api_url in apis:\n",
        "            try:\n",
        "                response = self.session.get(api_url, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    # Handle different API response formats\n",
        "                    if 'totalSolved' in data:  # herokuapp format\n",
        "                        return {\n",
        "                            \"Username\": username,\n",
        "                            \"Total Problems Solved\": data.get(\"totalSolved\", 0),\n",
        "                            \"Easy\": data.get(\"easySolved\", 0),\n",
        "                            \"Medium\": data.get(\"mediumSolved\", 0),\n",
        "                            \"Hard\": data.get(\"hardSolved\", 0),\n",
        "                            \"Contest Rating\": data.get(\"contestRating\", 0),\n",
        "                            \"Contests Attended\": data.get(\"contestAttend\", 0),\n",
        "                            \"Global Ranking\": data.get(\"ranking\", 'N/A'),\n",
        "                            \"Status\": f\"Success (API: {api_url.split('/')[2]})\"\n",
        "                        }\n",
        "                    elif 'solvedProblem' in data:  # alfa-leetcode format\n",
        "                        return {\n",
        "                            \"Username\": username,\n",
        "                            \"Total Problems Solved\": data.get(\"solvedProblem\", 0),\n",
        "                            \"Easy\": data.get(\"easySolved\", 0),\n",
        "                            \"Medium\": data.get(\"mediumSolved\", 0),\n",
        "                            \"Hard\": data.get(\"hardSolved\", 0),\n",
        "                            \"Contest Rating\": data.get(\"contestRating\", 0),\n",
        "                            \"Contests Attended\": data.get(\"contestParticipation\", 0),\n",
        "                            \"Global Ranking\": 'N/A',\n",
        "                            \"Status\": f\"Success (API: {api_url.split('/')[2]})\"\n",
        "                        }\n",
        "            except Exception as e:\n",
        "                print(f\"API {api_url} failed for {username}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    def scrape_profile(self, url):\n",
        "        \"\"\"Main scraping method that tries all approaches\"\"\"\n",
        "        username = url.strip(\"/\").split(\"/\")[-1]\n",
        "        print(f\"Scraping: {username}\")\n",
        "\n",
        "        # Method 1: Try GraphQL (most reliable for contest data)\n",
        "        result = self.scrape_with_graphql(username)\n",
        "        if result and result[\"Total Problems Solved\"] > 0:\n",
        "            return result\n",
        "\n",
        "        # Method 2: Try multiple third-party APIs\n",
        "        result = self.try_multiple_apis(username)\n",
        "        if result and result[\"Total Problems Solved\"] > 0:\n",
        "            return result\n",
        "\n",
        "        # Method 3: Direct HTML scraping\n",
        "        result = self.scrape_direct_html(username, url)\n",
        "        if result and result[\"Total Problems Solved\"] > 0:\n",
        "            return result\n",
        "\n",
        "        # If all methods fail\n",
        "        return {\n",
        "            \"Username\": username,\n",
        "            \"Total Problems Solved\": \"Failed\",\n",
        "            \"Easy\": \"Failed\",\n",
        "            \"Medium\": \"Failed\",\n",
        "            \"Hard\": \"Failed\",\n",
        "            \"Contest Rating\": \"Failed\",\n",
        "            \"Contests Attended\": \"Failed\",\n",
        "            \"Global Ranking\": \"Failed\",\n",
        "            \"Status\": \"All methods failed\"\n",
        "        }\n",
        "\n",
        "def scrape_all_profiles(profiles):\n",
        "    \"\"\"Scrape all profiles with progress tracking\"\"\"\n",
        "    scraper = LeetCodeScraper()\n",
        "    data = []\n",
        "    total = len(profiles)\n",
        "\n",
        "    for i, url in enumerate(profiles, 1):\n",
        "        print(f\"\\n--- Processing {i}/{total} ---\")\n",
        "        result = scraper.scrape_profile(url)\n",
        "        data.append(result)\n",
        "\n",
        "        # Show current result\n",
        "        print(f\"Result: {result['Status']}\")\n",
        "        if result[\"Total Problems Solved\"] != \"Failed\":\n",
        "            print(f\"  Total: {result['Total Problems Solved']}, Contest Rating: {result['Contest Rating']}\")\n",
        "\n",
        "        # Rate limiting\n",
        "        time.sleep(2)  # 2 second delay between requests\n",
        "\n",
        "        # Progress checkpoint\n",
        "        if i % 10 == 0:\n",
        "            print(f\"\\n=== Checkpoint: {i}/{total} completed ===\")\n",
        "            successful = sum(1 for r in data if r['Status'] != 'All methods failed')\n",
        "            print(f\"Success rate so far: {successful/i*100:.1f}%\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def export_to_pdf(df, filename=\"leetcode_report.pdf\"):\n",
        "    \"\"\"Export DataFrame to PDF with improved formatting\"\"\"\n",
        "    doc = SimpleDocTemplate(filename, pagesize=A4)\n",
        "    elements = []\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    # Title\n",
        "    elements.append(Paragraph(\"LeetCode Comprehensive Leaderboard Report\", styles['Title']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    # Summary statistics\n",
        "    total_users = len(df)\n",
        "    successful_scrapes = len(df[df['Status'].str.contains('Success', na=False)])\n",
        "    failed_scrapes = len(df[df['Status'].str.contains('Failed', na=False)])\n",
        "\n",
        "    # Calculate averages for successful scrapes\n",
        "    numeric_df = df[df['Status'].str.contains('Success', na=False)].copy()\n",
        "    for col in ['Total Problems Solved', 'Easy', 'Medium', 'Hard', 'Contest Rating', 'Contests Attended']:\n",
        "        numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')\n",
        "\n",
        "    avg_total = numeric_df['Total Problems Solved'].mean() if len(numeric_df) > 0 else 0\n",
        "    avg_rating = numeric_df['Contest Rating'].mean() if len(numeric_df) > 0 else 0\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    <b>Comprehensive Scraping Summary:</b><br/>\n",
        "    Total Users: {total_users}<br/>\n",
        "    Successfully Scraped: {successful_scrapes}<br/>\n",
        "    Failed Scrapes: {failed_scrapes}<br/>\n",
        "    Success Rate: {successful_scrapes/total_users*100:.1f}%<br/><br/>\n",
        "\n",
        "    <b>Performance Averages (Successful scrapes only):</b><br/>\n",
        "    Average Problems Solved: {avg_total:.1f}<br/>\n",
        "    Average Contest Rating: {avg_rating:.1f}<br/>\n",
        "    \"\"\"\n",
        "    elements.append(Paragraph(summary_text, styles['Normal']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    # Create table without Status column for cleaner output\n",
        "    display_df = df.drop(['Status'], axis=1, errors='ignore')\n",
        "    table_data = [display_df.columns.tolist()] + display_df.values.tolist()\n",
        "\n",
        "    table = Table(table_data, repeatRows=1)\n",
        "    table.setStyle(TableStyle([\n",
        "        (\"BACKGROUND\", (0,0), (-1,0), colors.darkblue),\n",
        "        (\"TEXTCOLOR\",(0,0),(-1,0),colors.whitesmoke),\n",
        "        (\"ALIGN\",(0,0),(-1,-1),\"CENTER\"),\n",
        "        (\"GRID\", (0,0), (-1,-1), 0.5, colors.black),\n",
        "        (\"FONTSIZE\", (0,0), (-1,-1), 6),\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"MIDDLE\"),\n",
        "        # Alternate row colors\n",
        "        (\"BACKGROUND\", (0,1), (-1,-1), colors.lightgrey),\n",
        "    ]))\n",
        "\n",
        "    elements.append(table)\n",
        "    doc.build(elements)\n",
        "\n",
        "# Main execution\n",
        "print(\"ðŸš€ Starting Comprehensive LeetCode Profile Scraping...\")\n",
        "print(\"This will try GraphQL, multiple APIs, and HTML scraping for maximum success rate!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Collect all profiles\n",
        "data = scrape_all_profiles(leetcode_profiles)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Detailed summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“Š FINAL SCRAPING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total profiles processed: {len(df)}\")\n",
        "\n",
        "# Status breakdown\n",
        "status_counts = df['Status'].value_counts()\n",
        "print(\"\\nDetailed status breakdown:\")\n",
        "for status, count in status_counts.items():\n",
        "    print(f\"  {status}: {count}\")\n",
        "\n",
        "# Success analysis\n",
        "successful_df = df[df['Status'].str.contains('Success', na=False)]\n",
        "if len(successful_df) > 0:\n",
        "    print(f\"\\nâœ… Successfully scraped {len(successful_df)} profiles!\")\n",
        "    print(\"\\nTop performers (by total problems solved):\")\n",
        "    try:\n",
        "        numeric_df = successful_df.copy()\n",
        "        numeric_df['Total Problems Solved'] = pd.to_numeric(numeric_df['Total Problems Solved'], errors='coerce')\n",
        "        top_5 = numeric_df.nlargest(5, 'Total Problems Solved')[['Username', 'Total Problems Solved', 'Contest Rating']]\n",
        "        print(top_5.to_string(index=False))\n",
        "    except:\n",
        "        print(\"Could not calculate top performers\")\n",
        "\n",
        "# Save comprehensive data\n",
        "df.to_csv('leetcode_comprehensive_data.csv', index=False)\n",
        "print(f\"\\nðŸ’¾ Comprehensive data saved to: leetcode_comprehensive_data.csv\")\n",
        "\n",
        "# Export to PDF\n",
        "export_to_pdf(df)\n",
        "print(\"ðŸ“„ PDF report generated: leetcode_report.pdf\")\n",
        "\n",
        "# Download files\n",
        "files.download(\"leetcode_report.pdf\")\n",
        "files.download(\"leetcode_comprehensive_data.csv\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Comprehensive scraping completed!\")\n",
        "print(\"Check the CSV file for detailed status information on each profile.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UzEEsypJd_9S",
        "outputId": "9e3659b0-5cb4-4df9-8e8f-87ed7fae9553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (4.35.0)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "ðŸš€ Starting Comprehensive LeetCode Profile Scraping...\n",
            "This will try GraphQL, multiple APIs, and HTML scraping for maximum success rate!\n",
            "============================================================\n",
            "\n",
            "--- Processing 1/65 ---\n",
            "Scraping: Divyansh_Joshi_MNIT\n",
            "Result: Success (GraphQL)\n",
            "  Total: 675, Contest Rating: 1606.4108077259166\n",
            "\n",
            "--- Processing 2/65 ---\n",
            "Scraping: Dhruv_parashar673\n",
            "Result: Success (GraphQL)\n",
            "  Total: 395, Contest Rating: 1519.1110157268558\n",
            "\n",
            "--- Processing 3/65 ---\n",
            "Scraping: shivansh_codes\n",
            "Result: Success (GraphQL)\n",
            "  Total: 233, Contest Rating: 0\n",
            "\n",
            "--- Processing 4/65 ---\n",
            "Scraping: hardik7427\n",
            "Result: Success (GraphQL)\n",
            "  Total: 413, Contest Rating: 1650.31219552124\n",
            "\n",
            "--- Processing 5/65 ---\n",
            "Scraping: Nayu_1501\n",
            "Result: Success (GraphQL)\n",
            "  Total: 143, Contest Rating: 1456.775544607062\n",
            "\n",
            "--- Processing 6/65 ---\n",
            "Scraping: mayank_kumar123\n",
            "Result: Success (GraphQL)\n",
            "  Total: 293, Contest Rating: 1452.581008814911\n",
            "\n",
            "--- Processing 7/65 ---\n",
            "Scraping: tushardhakad355\n",
            "Result: Success (GraphQL)\n",
            "  Total: 244, Contest Rating: 1347.9539414479702\n",
            "\n",
            "--- Processing 8/65 ---\n",
            "Scraping: rajatkhedar123\n",
            "Result: Success (GraphQL)\n",
            "  Total: 272, Contest Rating: 1512.1307036115472\n",
            "\n",
            "--- Processing 9/65 ---\n",
            "Scraping: Dishank_Jha\n",
            "Result: Success (GraphQL)\n",
            "  Total: 311, Contest Rating: 1486.627378918677\n",
            "\n",
            "--- Processing 10/65 ---\n",
            "Scraping: Khushal_Saini\n",
            "Result: Success (GraphQL)\n",
            "  Total: 531, Contest Rating: 1544.314836200915\n",
            "\n",
            "=== Checkpoint: 10/65 completed ===\n",
            "Success rate so far: 100.0%\n",
            "\n",
            "--- Processing 11/65 ---\n",
            "Scraping: mauliksidana09\n",
            "Result: Success (GraphQL)\n",
            "  Total: 552, Contest Rating: 1552.6498499659526\n",
            "\n",
            "--- Processing 12/65 ---\n",
            "Scraping: rudra_singh_07\n",
            "Result: Success (GraphQL)\n",
            "  Total: 229, Contest Rating: 1341.8752813612946\n",
            "\n",
            "--- Processing 13/65 ---\n",
            "Scraping: govindsingh_777\n",
            "Result: Success (GraphQL)\n",
            "  Total: 41, Contest Rating: 0\n",
            "\n",
            "--- Processing 14/65 ---\n",
            "Scraping: Vinay-Gupta\n",
            "Result: Success (GraphQL)\n",
            "  Total: 469, Contest Rating: 1564.0015329297378\n",
            "\n",
            "--- Processing 15/65 ---\n",
            "Scraping: Shivam_goyal_\n",
            "Result: Success (GraphQL)\n",
            "  Total: 131, Contest Rating: 1581.1634182883595\n",
            "\n",
            "--- Processing 16/65 ---\n",
            "Scraping: padmesh_0150\n",
            "Result: Success (GraphQL)\n",
            "  Total: 366, Contest Rating: 1431.936264038086\n",
            "\n",
            "--- Processing 17/65 ---\n",
            "Scraping: Gaurav_1810\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 18/65 ---\n",
            "Scraping: likhitd\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 19/65 ---\n",
            "Scraping: Divyanshverma15\n",
            "Result: Success (GraphQL)\n",
            "  Total: 181, Contest Rating: 0\n",
            "\n",
            "--- Processing 20/65 ---\n",
            "Scraping: kushagras_94\n",
            "Result: Success (GraphQL)\n",
            "  Total: 14, Contest Rating: 0\n",
            "\n",
            "=== Checkpoint: 20/65 completed ===\n",
            "Success rate so far: 90.0%\n",
            "\n",
            "--- Processing 21/65 ---\n",
            "Scraping: harshit3458\n",
            "Result: Success (GraphQL)\n",
            "  Total: 599, Contest Rating: 1555.1990145499692\n",
            "\n",
            "--- Processing 22/65 ---\n",
            "Scraping: RachitMittal1634\n",
            "Result: Success (GraphQL)\n",
            "  Total: 704, Contest Rating: 1699.2673803736782\n",
            "\n",
            "--- Processing 23/65 ---\n",
            "Scraping: saurabh_32\n",
            "Result: Success (GraphQL)\n",
            "  Total: 660, Contest Rating: 0\n",
            "\n",
            "--- Processing 24/65 ---\n",
            "Scraping: tehseen1639\n",
            "Result: Success (GraphQL)\n",
            "  Total: 339, Contest Rating: 1378.626247728309\n",
            "\n",
            "--- Processing 25/65 ---\n",
            "Scraping: ruchika_yadav\n",
            "Result: Success (GraphQL)\n",
            "  Total: 275, Contest Rating: 1522.876\n",
            "\n",
            "--- Processing 26/65 ---\n",
            "Scraping: yugsarda\n",
            "Result: Success (GraphQL)\n",
            "  Total: 615, Contest Rating: 1385.4071969012994\n",
            "\n",
            "--- Processing 27/65 ---\n",
            "Scraping: Kartik_Mahnot\n",
            "Result: Success (GraphQL)\n",
            "  Total: 579, Contest Rating: 1583.6453541125381\n",
            "\n",
            "--- Processing 28/65 ---\n",
            "Scraping: ViNiT-72\n",
            "Result: Success (GraphQL)\n",
            "  Total: 294, Contest Rating: 0\n",
            "\n",
            "--- Processing 29/65 ---\n",
            "Scraping: RK_Patel\n",
            "Result: Success (GraphQL)\n",
            "  Total: 1147, Contest Rating: 2039.9135632571758\n",
            "\n",
            "--- Processing 30/65 ---\n",
            "Scraping: beingKashvi\n",
            "Result: All methods failed\n",
            "\n",
            "=== Checkpoint: 30/65 completed ===\n",
            "Success rate so far: 90.0%\n",
            "\n",
            "--- Processing 31/65 ---\n",
            "Scraping: vikas_singh_856\n",
            "Result: Success (GraphQL)\n",
            "  Total: 465, Contest Rating: 1786.9131462158252\n",
            "\n",
            "--- Processing 32/65 ---\n",
            "Scraping: user2633T\n",
            "Result: Success (GraphQL)\n",
            "  Total: 269, Contest Rating: 1393.336061731491\n",
            "\n",
            "--- Processing 33/65 ---\n",
            "Scraping: ShourayaKaushik\n",
            "Result: Success (GraphQL)\n",
            "  Total: 110, Contest Rating: 0\n",
            "\n",
            "--- Processing 34/65 ---\n",
            "Scraping: jatin-agrawal\n",
            "Result: Success (GraphQL)\n",
            "  Total: 771, Contest Rating: 1622.9624352589017\n",
            "\n",
            "--- Processing 35/65 ---\n",
            "Scraping: Bot-Netizen-Programmers\n",
            "Result: Success (GraphQL)\n",
            "  Total: 3, Contest Rating: 0\n",
            "\n",
            "--- Processing 36/65 ---\n",
            "Scraping: bhargav-1673\n",
            "Result: Success (GraphQL)\n",
            "  Total: 53, Contest Rating: 0\n",
            "\n",
            "--- Processing 37/65 ---\n",
            "Scraping: Lakshit_Ramani\n",
            "Result: Success (GraphQL)\n",
            "  Total: 377, Contest Rating: 1413.731575012207\n",
            "\n",
            "--- Processing 38/65 ---\n",
            "Scraping: dhakad_09\n",
            "Result: Success (GraphQL)\n",
            "  Total: 734, Contest Rating: 1666.1319479342194\n",
            "\n",
            "--- Processing 39/65 ---\n",
            "Scraping: NAregarded\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 40/65 ---\n",
            "Scraping: akshaypal_bishnoi\n",
            "Result: Success (GraphQL)\n",
            "  Total: 237, Contest Rating: 1460.0191046443483\n",
            "\n",
            "=== Checkpoint: 40/65 completed ===\n",
            "Success rate so far: 90.0%\n",
            "\n",
            "--- Processing 41/65 ---\n",
            "Scraping: godika_priya\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 42/65 ---\n",
            "Scraping: Tush1586\n",
            "Result: Success (GraphQL)\n",
            "  Total: 375, Contest Rating: 1524.71923828125\n",
            "\n",
            "--- Processing 43/65 ---\n",
            "Scraping: Sarvesh25\n",
            "Result: Success (GraphQL)\n",
            "  Total: 472, Contest Rating: 1481.4829346318322\n",
            "\n",
            "--- Processing 44/65 ---\n",
            "Scraping: user8201yw\n",
            "Result: Success (GraphQL)\n",
            "  Total: 298, Contest Rating: 1412.485299115005\n",
            "\n",
            "--- Processing 45/65 ---\n",
            "Scraping: Swayam_141\n",
            "Result: Success (GraphQL)\n",
            "  Total: 162, Contest Rating: 1592.259407043457\n",
            "\n",
            "--- Processing 46/65 ---\n",
            "Scraping: Kashishgarg_15\n",
            "Result: Success (GraphQL)\n",
            "  Total: 440, Contest Rating: 1411.487\n",
            "\n",
            "--- Processing 47/65 ---\n",
            "Scraping: princimantri_2990\n",
            "Result: Success (GraphQL)\n",
            "  Total: 520, Contest Rating: 1589.461326599121\n",
            "\n",
            "--- Processing 48/65 ---\n",
            "Scraping: ankit4092004_Ankit___\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 49/65 ---\n",
            "Scraping: pritamzzziscodingpranav_1686\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 50/65 ---\n",
            "Scraping: Dhruvik_MYI\n",
            "Result: Success (GraphQL)\n",
            "  Total: 775, Contest Rating: 1455.6879200375206\n",
            "\n",
            "=== Checkpoint: 50/65 completed ===\n",
            "Success rate so far: 86.0%\n",
            "\n",
            "--- Processing 51/65 ---\n",
            "Scraping: pankaj1213\n",
            "Result: Success (GraphQL)\n",
            "  Total: 251, Contest Rating: 1465.042338506708\n",
            "\n",
            "--- Processing 52/65 ---\n",
            "Scraping: kavya1502_\n",
            "Result: Success (GraphQL)\n",
            "  Total: 490, Contest Rating: 1608.8863172029194\n",
            "\n",
            "--- Processing 53/65 ---\n",
            "Scraping: suhaani17\n",
            "Result: Success (GraphQL)\n",
            "  Total: 197, Contest Rating: 1459.0292478862561\n",
            "\n",
            "--- Processing 54/65 ---\n",
            "Scraping: Dikshit_Rao\n",
            "Result: Success (GraphQL)\n",
            "  Total: 199, Contest Rating: 1548.508644104004\n",
            "\n",
            "--- Processing 55/65 ---\n",
            "Scraping: krishnasharma1234\n",
            "Result: Success (GraphQL)\n",
            "  Total: 501, Contest Rating: 1490.6314558499446\n",
            "\n",
            "--- Processing 56/65 ---\n",
            "Scraping: KRITI_BHATNAGAR\n",
            "Result: Success (GraphQL)\n",
            "  Total: 599, Contest Rating: 1676.8496517542872\n",
            "\n",
            "--- Processing 57/65 ---\n",
            "Scraping: Roshan_nama12\n",
            "Result: Success (GraphQL)\n",
            "  Total: 35, Contest Rating: 0\n",
            "\n",
            "--- Processing 58/65 ---\n",
            "Scraping: gautam_chauhan04\n",
            "Result: Success (GraphQL)\n",
            "  Total: 81, Contest Rating: 1438.3353183144018\n",
            "\n",
            "--- Processing 59/65 ---\n",
            "Scraping: ayushkumar85371\n",
            "Result: Success (GraphQL)\n",
            "  Total: 296, Contest Rating: 1293.9775313282935\n",
            "\n",
            "--- Processing 60/65 ---\n",
            "Scraping: Gulab_Rana\n",
            "Result: Success (GraphQL)\n",
            "  Total: 42, Contest Rating: 0\n",
            "\n",
            "=== Checkpoint: 60/65 completed ===\n",
            "Success rate so far: 88.3%\n",
            "\n",
            "--- Processing 61/65 ---\n",
            "Scraping: utkarshmodi10\n",
            "Result: Success (GraphQL)\n",
            "  Total: 19, Contest Rating: 0\n",
            "\n",
            "--- Processing 62/65 ---\n",
            "Scraping: Yash_07___077\n",
            "Result: Success (GraphQL)\n",
            "  Total: 209, Contest Rating: 1373.7017580141508\n",
            "\n",
            "--- Processing 63/65 ---\n",
            "Scraping: MJ_Champ\n",
            "Result: Success (GraphQL)\n",
            "  Total: 800, Contest Rating: 1813.3601248854955\n",
            "\n",
            "--- Processing 64/65 ---\n",
            "Scraping: AyushTak\n",
            "Result: Success (GraphQL)\n",
            "  Total: 144, Contest Rating: 0\n",
            "\n",
            "--- Processing 65/65 ---\n",
            "Scraping: SIY7gDUBSu\n",
            "Result: All methods failed\n",
            "\n",
            "============================================================\n",
            "ðŸ“Š FINAL SCRAPING SUMMARY\n",
            "============================================================\n",
            "Total profiles processed: 65\n",
            "\n",
            "Detailed status breakdown:\n",
            "  Success (GraphQL): 57\n",
            "  All methods failed: 8\n",
            "\n",
            "âœ… Successfully scraped 57 profiles!\n",
            "\n",
            "Top performers (by total problems solved):\n",
            "     Username  Total Problems Solved Contest Rating\n",
            "     RK_Patel                   1147    2039.913563\n",
            "     MJ_Champ                    800    1813.360125\n",
            "  Dhruvik_MYI                    775     1455.68792\n",
            "jatin-agrawal                    771    1622.962435\n",
            "    dhakad_09                    734    1666.131948\n",
            "\n",
            "ðŸ’¾ Comprehensive data saved to: leetcode_comprehensive_data.csv\n",
            "ðŸ“„ PDF report generated: leetcode_report.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_afb3e14c-01b4-4ead-8ce6-6e4664e8e0e0\", \"leetcode_report.pdf\", 9087)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_43654b6f-54e9-450e-b398-6ad773a456de\", \"leetcode_comprehensive_data.csv\", 4660)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ‰ Comprehensive scraping completed!\n",
            "Check the CSV file for detailed status information on each profile.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab requests pandas beautifulsoup4\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from google.colab import files\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Add your CodeForces profile URLs here\n",
        "codeforces_profiles = [\n",
        "     \"https://codeforces.com/profile/Divyansh_Joshi\",\n",
        "    \"https://codeforces.com/profile/2023uai1820\",\n",
        "    \"https://codeforces.com/profile/hardikagrawal742700\",\n",
        "    \"https://codeforces.com/profile/mayankrajxxxxx5\",\n",
        "    \"https://codeforces.com/profile/rajatkhedar123\",\n",
        "    \"https://codeforces.com/profile/Dishank_Jha\",\n",
        "    \"https://codeforces.com/profile/Rudra_Singh_07\",\n",
        "    \"https://codeforces.com/profile/VinayGupta28\",\n",
        "    \"https://codeforces.com/profile/Shivam_goyal_00\",\n",
        "    \"https://codeforces.com/profile/Padmesh\",\n",
        "    \"https://codeforces.com/profile/harshit3458\",\n",
        "    \"https://codeforces.com/profile/kartikmahnot1\",\n",
        "    \"https://codeforces.com/profile/VIKAS__SINGH\",\n",
        "    \"https://codeforces.com/profile/jatinagrawal0987654321\",\n",
        "    \"https://codeforces.com/profile/2023ume1931\",\n",
        "    \"https://codeforces.com/profile/dhakadgulshankumar\",\n",
        "    \"https://codeforces.com/profile/akshay_20_9_e\",\n",
        "    \"https://codeforces.com/profile/sarvesh22210\",\n",
        "    \"https://codeforces.com/profile/Kashishgarg_15\",\n",
        "    \"https://codeforces.com/profile/ankitkr.code\",\n",
        "    \"https://codeforces.com/profile/pritamzzziscodingpranav1620kumar\",\n",
        "    \"https://codeforces.com/profile/Pankaj1275\",\n",
        "    \"https://codeforces.com/profile/kavya15022006\",\n",
        "    \"https://codeforces.com/profile/suhaani17\",\n",
        "    \"https://codeforces.com/profile/dikshit_789\",\n",
        "    \"https://codeforces.com/profile/krishna7777\",\n",
        "    \"https://codeforces.com/profile/KritiB\",\n",
        "    \"https://codeforces.com/profile/Roshan_nama\",\n",
        "    \"https://codeforces.com/profile/ayushkumar85371\",\n",
        "    \"https://codeforces.com/profile/yashpratap291\",\n",
        "    \"https://codeforces.com/profile/Thanwal\",\n",
        "    \"https://codeforces.com/profile/abhi021\",\n",
        "    \"https://codeforces.com/profile/Sy4aug2004\",\n",
        "    \"https://codeforces.com/profile/abhinavguptaxia9213\",\n",
        "    \"https://codeforces.com/profile/2023uec1031\"\n",
        "    # Add your student profiles here\n",
        "    # Example format: \"https://codeforces.com/profile/username\"\n",
        "]\n",
        "\n",
        "class CodeForcesScraper:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'application/json, text/plain, */*',\n",
        "            'Accept-Language': 'en-US,en;q=0.9',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Referer': 'https://codeforces.com/'\n",
        "        })\n",
        "\n",
        "        # CodeForces rating ranges and colors\n",
        "        self.rating_colors = {\n",
        "            (0, 1199): (\"Newbie\", \"gray\"),\n",
        "            (1200, 1399): (\"Pupil\", \"green\"),\n",
        "            (1400, 1599): (\"Specialist\", \"cyan\"),\n",
        "            (1600, 1899): (\"Expert\", \"blue\"),\n",
        "            (1900, 2099): (\"Candidate Master\", \"violet\"),\n",
        "            (2100, 2299): (\"Master\", \"orange\"),\n",
        "            (2300, 2399): (\"International Master\", \"orange\"),\n",
        "            (2400, 2599): (\"Grandmaster\", \"red\"),\n",
        "            (2600, 2999): (\"International Grandmaster\", \"red\"),\n",
        "            (3000, 9999): (\"Legendary Grandmaster\", \"red\")\n",
        "        }\n",
        "\n",
        "    def get_rating_title(self, rating):\n",
        "        \"\"\"Get the title based on rating\"\"\"\n",
        "        if rating is None or rating == 0:\n",
        "            return \"Unrated\"\n",
        "\n",
        "        for (min_rating, max_rating), (title, color) in self.rating_colors.items():\n",
        "            if min_rating <= rating <= max_rating:\n",
        "                return title\n",
        "        return \"Unknown\"\n",
        "\n",
        "    def scrape_with_official_api(self, username):\n",
        "        \"\"\"Use CodeForces official API - most reliable method\"\"\"\n",
        "        try:\n",
        "            # Get user info\n",
        "            user_info_url = f\"https://codeforces.com/api/user.info?handles={username}\"\n",
        "            response = self.session.get(user_info_url, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "\n",
        "                if data.get('status') == 'OK' and data.get('result'):\n",
        "                    user = data['result'][0]\n",
        "\n",
        "                    # Get user statistics\n",
        "                    stats_url = f\"https://codeforces.com/api/user.status?handle={username}&from=1&count=10000\"\n",
        "                    stats_response = self.session.get(stats_url, timeout=15)\n",
        "\n",
        "                    problem_stats = self.analyze_submissions(stats_response.json() if stats_response.status_code == 200 else {})\n",
        "\n",
        "                    return {\n",
        "                        \"Username\": username,\n",
        "                        \"Real Name\": user.get('firstName', '') + ' ' + user.get('lastName', ''),\n",
        "                        \"Country\": user.get('country', 'N/A'),\n",
        "                        \"City\": user.get('city', 'N/A'),\n",
        "                        \"Organization\": user.get('organization', 'N/A'),\n",
        "                        \"Current Rating\": user.get('rating', 0),\n",
        "                        \"Max Rating\": user.get('maxRating', 0),\n",
        "                        \"Rank\": user.get('rank', 'unrated'),\n",
        "                        \"Max Rank\": user.get('maxRank', 'unrated'),\n",
        "                        \"Title\": self.get_rating_title(user.get('rating', 0)),\n",
        "                        \"Problems Solved\": problem_stats['solved_count'],\n",
        "                        \"Total Submissions\": problem_stats['total_submissions'],\n",
        "                        \"Accepted Submissions\": problem_stats['accepted_submissions'],\n",
        "                        \"Success Rate\": f\"{problem_stats['success_rate']:.1f}%\",\n",
        "                        \"Contests Participated\": user.get('contribution', 0),  # Approximate\n",
        "                        \"Last Online\": self.format_timestamp(user.get('lastOnlineTimeSeconds', 0)),\n",
        "                        \"Registration Date\": self.format_timestamp(user.get('registrationTimeSeconds', 0)),\n",
        "                        \"Status\": \"Success (Official API)\"\n",
        "                    }\n",
        "                else:\n",
        "                    print(f\"API returned error for {username}: {data.get('comment', 'Unknown error')}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Official API failed for {username}: {str(e)}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def analyze_submissions(self, stats_data):\n",
        "        \"\"\"Analyze submission statistics\"\"\"\n",
        "        result = {\n",
        "            'solved_count': 0,\n",
        "            'total_submissions': 0,\n",
        "            'accepted_submissions': 0,\n",
        "            'success_rate': 0,\n",
        "            'problem_difficulties': defaultdict(int)\n",
        "        }\n",
        "\n",
        "        if stats_data.get('status') != 'OK' or not stats_data.get('result'):\n",
        "            return result\n",
        "\n",
        "        submissions = stats_data['result']\n",
        "        solved_problems = set()\n",
        "\n",
        "        for submission in submissions:\n",
        "            result['total_submissions'] += 1\n",
        "\n",
        "            if submission.get('verdict') == 'OK':\n",
        "                result['accepted_submissions'] += 1\n",
        "\n",
        "                # Count unique solved problems\n",
        "                problem_key = (submission['problem']['contestId'], submission['problem']['index'])\n",
        "                solved_problems.add(problem_key)\n",
        "\n",
        "                # Count by difficulty/rating\n",
        "                if 'rating' in submission['problem']:\n",
        "                    rating = submission['problem']['rating']\n",
        "                    result['problem_difficulties'][rating] += 1\n",
        "\n",
        "        result['solved_count'] = len(solved_problems)\n",
        "        result['success_rate'] = (result['accepted_submissions'] / result['total_submissions'] * 100) if result['total_submissions'] > 0 else 0\n",
        "\n",
        "        return result\n",
        "\n",
        "    def format_timestamp(self, timestamp):\n",
        "        \"\"\"Format Unix timestamp to readable date\"\"\"\n",
        "        if timestamp and timestamp > 0:\n",
        "            try:\n",
        "                import datetime\n",
        "                return datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
        "            except:\n",
        "                return 'N/A'\n",
        "        return 'N/A'\n",
        "\n",
        "    def scrape_contest_history(self, username):\n",
        "        \"\"\"Get contest participation history\"\"\"\n",
        "        try:\n",
        "            contests_url = f\"https://codeforces.com/api/user.rating?handle={username}\"\n",
        "            response = self.session.get(contests_url, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "\n",
        "                if data.get('status') == 'OK' and data.get('result'):\n",
        "                    contests = data['result']\n",
        "\n",
        "                    return {\n",
        "                        'contests_count': len(contests),\n",
        "                        'best_rank': min([c.get('rank', 999999) for c in contests]) if contests else 0,\n",
        "                        'worst_rank': max([c.get('rank', 0) for c in contests]) if contests else 0,\n",
        "                        'rating_changes': len([c for c in contests if c.get('newRating', 0) > c.get('oldRating', 0)]),\n",
        "                        'latest_contest': contests[-1]['contestName'] if contests else 'N/A'\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            print(f\"Contest history failed for {username}: {str(e)}\")\n",
        "\n",
        "        return {\n",
        "            'contests_count': 0,\n",
        "            'best_rank': 'N/A',\n",
        "            'worst_rank': 'N/A',\n",
        "            'rating_changes': 'N/A',\n",
        "            'latest_contest': 'N/A'\n",
        "        }\n",
        "\n",
        "    def scrape_html_fallback(self, username, url):\n",
        "        \"\"\"Fallback HTML scraping method\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                result = {\n",
        "                    \"Username\": username,\n",
        "                    \"Real Name\": 'N/A',\n",
        "                    \"Country\": 'N/A',\n",
        "                    \"City\": 'N/A',\n",
        "                    \"Organization\": 'N/A',\n",
        "                    \"Current Rating\": 0,\n",
        "                    \"Max Rating\": 0,\n",
        "                    \"Rank\": 'unrated',\n",
        "                    \"Max Rank\": 'unrated',\n",
        "                    \"Title\": 'Unrated',\n",
        "                    \"Problems Solved\": 0,\n",
        "                    \"Total Submissions\": 0,\n",
        "                    \"Accepted Submissions\": 0,\n",
        "                    \"Success Rate\": \"0%\",\n",
        "                    \"Contests Participated\": 0,\n",
        "                    \"Last Online\": 'N/A',\n",
        "                    \"Registration Date\": 'N/A',\n",
        "                    \"Status\": \"HTML Scraping\"\n",
        "                }\n",
        "\n",
        "                # Look for rating information\n",
        "                rating_elements = soup.find_all(text=re.compile(r'\\d{4}'))\n",
        "                for elem in rating_elements:\n",
        "                    if elem.parent and 'rating' in elem.parent.get('class', []):\n",
        "                        try:\n",
        "                            rating = int(elem.strip())\n",
        "                            if 800 <= rating <= 4000:  # Valid CodeForces rating range\n",
        "                                result[\"Current Rating\"] = rating\n",
        "                                result[\"Title\"] = self.get_rating_title(rating)\n",
        "                                break\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                # Look for solved problems count\n",
        "                text_content = soup.get_text()\n",
        "                solved_match = re.search(r'(\\d+)\\s*problems?\\s*solved', text_content, re.IGNORECASE)\n",
        "                if solved_match:\n",
        "                    result[\"Problems Solved\"] = int(solved_match.group(1))\n",
        "\n",
        "                # Look for submissions\n",
        "                submissions_match = re.search(r'(\\d+)\\s*submissions?', text_content, re.IGNORECASE)\n",
        "                if submissions_match:\n",
        "                    result[\"Total Submissions\"] = int(submissions_match.group(1))\n",
        "\n",
        "                return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"HTML scraping failed for {username}: {str(e)}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def scrape_profile(self, url):\n",
        "        \"\"\"Main scraping method\"\"\"\n",
        "        username = url.strip(\"/\").split(\"/\")[-1]\n",
        "        print(f\"Scraping: {username}\")\n",
        "\n",
        "        # Method 1: Official CodeForces API (most reliable)\n",
        "        result = self.scrape_with_official_api(username)\n",
        "        if result:\n",
        "            # Enhance with contest history\n",
        "            contest_data = self.scrape_contest_history(username)\n",
        "            result.update({\n",
        "                \"Total Contests\": contest_data['contests_count'],\n",
        "                \"Best Rank\": contest_data['best_rank'],\n",
        "                \"Latest Contest\": contest_data['latest_contest']\n",
        "            })\n",
        "            return result\n",
        "\n",
        "        # Method 2: HTML scraping fallback\n",
        "        result = self.scrape_html_fallback(username, url)\n",
        "        if result and result[\"Current Rating\"] > 0:\n",
        "            return result\n",
        "\n",
        "        # If all methods fail\n",
        "        return {\n",
        "            \"Username\": username,\n",
        "            \"Real Name\": \"Failed\",\n",
        "            \"Country\": \"Failed\",\n",
        "            \"Current Rating\": \"Failed\",\n",
        "            \"Max Rating\": \"Failed\",\n",
        "            \"Rank\": \"Failed\",\n",
        "            \"Title\": \"Failed\",\n",
        "            \"Problems Solved\": \"Failed\",\n",
        "            \"Total Submissions\": \"Failed\",\n",
        "            \"Success Rate\": \"Failed\",\n",
        "            \"Total Contests\": \"Failed\",\n",
        "            \"Status\": \"All methods failed\"\n",
        "        }\n",
        "\n",
        "def scrape_all_profiles(profiles):\n",
        "    \"\"\"Scrape all CodeForces profiles\"\"\"\n",
        "    scraper = CodeForcesScraper()\n",
        "    data = []\n",
        "    total = len(profiles)\n",
        "\n",
        "    for i, url in enumerate(profiles, 1):\n",
        "        print(f\"\\n--- Processing {i}/{total} ---\")\n",
        "        result = scraper.scrape_profile(url)\n",
        "        data.append(result)\n",
        "\n",
        "        # Show current result\n",
        "        print(f\"Result: {result['Status']}\")\n",
        "        if result.get(\"Current Rating\") != \"Failed\":\n",
        "            print(f\"  Rating: {result.get('Current Rating', 0)} ({result.get('Title', 'N/A')})\")\n",
        "            print(f\"  Problems: {result.get('Problems Solved', 0)}\")\n",
        "\n",
        "        # Rate limiting - CodeForces API allows more frequent requests\n",
        "        time.sleep(1)  # 1 second delay\n",
        "\n",
        "        # Progress checkpoint\n",
        "        if i % 10 == 0:\n",
        "            print(f\"\\n=== Checkpoint: {i}/{total} completed ===\")\n",
        "            successful = sum(1 for r in data if r['Status'] != 'All methods failed')\n",
        "            print(f\"Success rate so far: {successful/i*100:.1f}%\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def export_to_pdf(df, filename=\"codeforces_report.pdf\"):\n",
        "    \"\"\"Export DataFrame to PDF with CodeForces styling\"\"\"\n",
        "    doc = SimpleDocTemplate(filename, pagesize=A4)\n",
        "    elements = []\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    # Title\n",
        "    elements.append(Paragraph(\"CodeForces Comprehensive Leaderboard Report\", styles['Title']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    # Summary statistics\n",
        "    total_users = len(df)\n",
        "    successful_scrapes = len(df[df['Status'].str.contains('Success', na=False)])\n",
        "    failed_scrapes = len(df[df['Status'].str.contains('Failed', na=False)])\n",
        "\n",
        "    # Calculate statistics for successful scrapes\n",
        "    numeric_df = df[df['Status'].str.contains('Success', na=False)].copy()\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_cols = ['Current Rating', 'Max Rating', 'Problems Solved', 'Total Submissions']\n",
        "    for col in numeric_cols:\n",
        "        if col in numeric_df.columns:\n",
        "            numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_rating = numeric_df['Current Rating'].mean() if len(numeric_df) > 0 and 'Current Rating' in numeric_df.columns else 0\n",
        "    avg_problems = numeric_df['Problems Solved'].mean() if len(numeric_df) > 0 and 'Problems Solved' in numeric_df.columns else 0\n",
        "\n",
        "    # Count by titles\n",
        "    title_counts = numeric_df['Title'].value_counts().to_dict() if 'Title' in numeric_df.columns else {}\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    <b>CodeForces Scraping Summary:</b><br/>\n",
        "    Total Users: {total_users}<br/>\n",
        "    Successfully Scraped: {successful_scrapes}<br/>\n",
        "    Failed Scrapes: {failed_scrapes}<br/>\n",
        "    Success Rate: {successful_scrapes/total_users*100:.1f}%<br/><br/>\n",
        "\n",
        "    <b>Performance Statistics:</b><br/>\n",
        "    Average Current Rating: {avg_rating:.0f}<br/>\n",
        "    Average Problems Solved: {avg_problems:.0f}<br/><br/>\n",
        "\n",
        "    <b>Title Distribution:</b><br/>\n",
        "    \"\"\"\n",
        "\n",
        "    for title, count in sorted(title_counts.items()):\n",
        "        summary_text += f\"{title}: {count}<br/>\"\n",
        "\n",
        "    elements.append(Paragraph(summary_text, styles['Normal']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    # Create main data table\n",
        "    display_cols = ['Username', 'Current Rating', 'Max Rating', 'Title', 'Problems Solved',\n",
        "                   'Total Submissions', 'Success Rate', 'Total Contests', 'Country']\n",
        "\n",
        "    # Filter columns that exist in the dataframe\n",
        "    available_cols = [col for col in display_cols if col in df.columns]\n",
        "    display_df = df[available_cols]\n",
        "\n",
        "    table_data = [display_df.columns.tolist()] + display_df.values.tolist()\n",
        "\n",
        "    table = Table(table_data, repeatRows=1)\n",
        "    table.setStyle(TableStyle([\n",
        "        (\"BACKGROUND\", (0,0), (-1,0), colors.darkred),  # CodeForces red theme\n",
        "        (\"TEXTCOLOR\",(0,0),(-1,0),colors.whitesmoke),\n",
        "        (\"ALIGN\",(0,0),(-1,-1),\"CENTER\"),\n",
        "        (\"GRID\", (0,0), (-1,-1), 0.5, colors.black),\n",
        "        (\"FONTSIZE\", (0,0), (-1,-1), 6),\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"MIDDLE\"),\n",
        "        # Alternate row colors\n",
        "        (\"BACKGROUND\", (0,1), (-1,-1), colors.lightgrey),\n",
        "    ]))\n",
        "\n",
        "    elements.append(table)\n",
        "    doc.build(elements)\n",
        "\n",
        "# Main execution\n",
        "print(\"ðŸš€ Starting Comprehensive CodeForces Profile Scraping...\")\n",
        "print(\"Using official CodeForces API for maximum accuracy!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Validate URLs\n",
        "print(\"Validating profile URLs...\")\n",
        "valid_profiles = []\n",
        "for url in codeforces_profiles:\n",
        "    if 'codeforces.com/profile/' in url:\n",
        "        valid_profiles.append(url)\n",
        "    else:\n",
        "        print(f\"âš ï¸  Invalid URL format: {url}\")\n",
        "\n",
        "if not valid_profiles:\n",
        "    print(\"âŒ No valid CodeForces profile URLs found!\")\n",
        "    print(\"Please add URLs in the format: https://codeforces.com/profile/username\")\n",
        "else:\n",
        "    print(f\"âœ… Found {len(valid_profiles)} valid profile URLs\")\n",
        "\n",
        "    # Collect all profiles\n",
        "    data = scrape_all_profiles(valid_profiles)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Detailed summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ“Š FINAL CODEFORCES SCRAPING SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total profiles processed: {len(df)}\")\n",
        "\n",
        "    # Status breakdown\n",
        "    if 'Status' in df.columns:\n",
        "        status_counts = df['Status'].value_counts()\n",
        "        print(\"\\nDetailed status breakdown:\")\n",
        "        for status, count in status_counts.items():\n",
        "            print(f\"  {status}: {count}\")\n",
        "\n",
        "    # Success analysis\n",
        "    successful_df = df[df['Status'].str.contains('Success', na=False)] if 'Status' in df.columns else df\n",
        "\n",
        "    if len(successful_df) > 0:\n",
        "        print(f\"\\nâœ… Successfully scraped {len(successful_df)} profiles!\")\n",
        "\n",
        "        # Top performers by rating\n",
        "        if 'Current Rating' in successful_df.columns:\n",
        "            print(\"\\nðŸ† Top performers (by current rating):\")\n",
        "            try:\n",
        "                numeric_df = successful_df.copy()\n",
        "                numeric_df['Current Rating'] = pd.to_numeric(numeric_df['Current Rating'], errors='coerce')\n",
        "                top_5 = numeric_df.nlargest(5, 'Current Rating')[['Username', 'Current Rating', 'Title', 'Problems Solved']]\n",
        "                print(top_5.to_string(index=False))\n",
        "            except Exception as e:\n",
        "                print(f\"Could not calculate top performers: {e}\")\n",
        "\n",
        "        # Title distribution\n",
        "        if 'Title' in successful_df.columns:\n",
        "            print(\"\\nðŸŽ¯ Title Distribution:\")\n",
        "            title_dist = successful_df['Title'].value_counts()\n",
        "            for title, count in title_dist.items():\n",
        "                print(f\"  {title}: {count}\")\n",
        "\n",
        "    # Save comprehensive data\n",
        "    df.to_csv('codeforces_comprehensive_data.csv', index=False)\n",
        "    print(f\"\\nðŸ’¾ Comprehensive data saved to: codeforces_comprehensive_data.csv\")\n",
        "\n",
        "    # Export to PDF\n",
        "    export_to_pdf(df)\n",
        "    print(\"ðŸ“„ PDF report generated: codeforces_report.pdf\")\n",
        "\n",
        "    # Download files\n",
        "    files.download(\"codeforces_report.pdf\")\n",
        "    files.download(\"codeforces_comprehensive_data.csv\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ CodeForces scraping completed successfully!\")\n",
        "    print(\"The CSV contains all detailed information including registration dates, countries, etc.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fMQOYPfFqKqD",
        "outputId": "c648973c-1a99-4c81-b444-dfba7bcae538"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading reportlab-4.4.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.4.3\n",
            "ðŸš€ Starting Comprehensive CodeForces Profile Scraping...\n",
            "Using official CodeForces API for maximum accuracy!\n",
            "======================================================================\n",
            "Validating profile URLs...\n",
            "âœ… Found 35 valid profile URLs\n",
            "\n",
            "--- Processing 1/35 ---\n",
            "Scraping: Divyansh_Joshi\n",
            "Result: Success (Official API)\n",
            "  Rating: 1225 (Pupil)\n",
            "  Problems: 165\n",
            "\n",
            "--- Processing 2/35 ---\n",
            "Scraping: 2023uai1820\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 4\n",
            "\n",
            "--- Processing 3/35 ---\n",
            "Scraping: hardikagrawal742700\n",
            "Result: Success (Official API)\n",
            "  Rating: 700 (Newbie)\n",
            "  Problems: 27\n",
            "\n",
            "--- Processing 4/35 ---\n",
            "Scraping: mayankrajxxxxx5\n",
            "Result: Success (Official API)\n",
            "  Rating: 589 (Newbie)\n",
            "  Problems: 4\n",
            "\n",
            "--- Processing 5/35 ---\n",
            "Scraping: rajatkhedar123\n",
            "Result: Success (Official API)\n",
            "  Rating: 795 (Newbie)\n",
            "  Problems: 41\n",
            "\n",
            "--- Processing 6/35 ---\n",
            "Scraping: Dishank_Jha\n",
            "Result: Success (Official API)\n",
            "  Rating: 1019 (Newbie)\n",
            "  Problems: 248\n",
            "\n",
            "--- Processing 7/35 ---\n",
            "Scraping: Rudra_Singh_07\n",
            "Result: Success (Official API)\n",
            "  Rating: 980 (Newbie)\n",
            "  Problems: 15\n",
            "\n",
            "--- Processing 8/35 ---\n",
            "Scraping: VinayGupta28\n",
            "Result: Success (Official API)\n",
            "  Rating: 959 (Newbie)\n",
            "  Problems: 62\n",
            "\n",
            "--- Processing 9/35 ---\n",
            "Scraping: Shivam_goyal_00\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 10/35 ---\n",
            "Scraping: Padmesh\n",
            "Result: Success (Official API)\n",
            "  Rating: 1433 (Specialist)\n",
            "  Problems: 595\n",
            "\n",
            "=== Checkpoint: 10/35 completed ===\n",
            "Success rate so far: 90.0%\n",
            "\n",
            "--- Processing 11/35 ---\n",
            "Scraping: harshit3458\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 0\n",
            "\n",
            "--- Processing 12/35 ---\n",
            "Scraping: kartikmahnot1\n",
            "Result: Success (Official API)\n",
            "  Rating: 1115 (Newbie)\n",
            "  Problems: 239\n",
            "\n",
            "--- Processing 13/35 ---\n",
            "Scraping: VIKAS__SINGH\n",
            "Result: Success (Official API)\n",
            "  Rating: 1201 (Pupil)\n",
            "  Problems: 20\n",
            "\n",
            "--- Processing 14/35 ---\n",
            "Scraping: jatinagrawal0987654321\n",
            "Result: Success (Official API)\n",
            "  Rating: 1048 (Newbie)\n",
            "  Problems: 45\n",
            "\n",
            "--- Processing 15/35 ---\n",
            "Scraping: 2023ume1931\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 0\n",
            "\n",
            "--- Processing 16/35 ---\n",
            "Scraping: dhakadgulshankumar\n",
            "Result: Success (Official API)\n",
            "  Rating: 1139 (Newbie)\n",
            "  Problems: 302\n",
            "\n",
            "--- Processing 17/35 ---\n",
            "Scraping: akshay_20_9_e\n",
            "Result: Success (Official API)\n",
            "  Rating: 837 (Newbie)\n",
            "  Problems: 5\n",
            "\n",
            "--- Processing 18/35 ---\n",
            "Scraping: sarvesh22210\n",
            "Result: Success (Official API)\n",
            "  Rating: 1022 (Newbie)\n",
            "  Problems: 9\n",
            "\n",
            "--- Processing 19/35 ---\n",
            "Scraping: Kashishgarg_15\n",
            "Result: Success (Official API)\n",
            "  Rating: 373 (Newbie)\n",
            "  Problems: 16\n",
            "\n",
            "--- Processing 20/35 ---\n",
            "Scraping: ankitkr.code\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 0\n",
            "\n",
            "=== Checkpoint: 20/35 completed ===\n",
            "Success rate so far: 95.0%\n",
            "\n",
            "--- Processing 21/35 ---\n",
            "Scraping: pritamzzziscodingpranav1620kumar\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 22/35 ---\n",
            "Scraping: Pankaj1275\n",
            "Result: Success (Official API)\n",
            "  Rating: 360 (Newbie)\n",
            "  Problems: 14\n",
            "\n",
            "--- Processing 23/35 ---\n",
            "Scraping: kavya15022006\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 1\n",
            "\n",
            "--- Processing 24/35 ---\n",
            "Scraping: suhaani17\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 2\n",
            "\n",
            "--- Processing 25/35 ---\n",
            "Scraping: dikshit_789\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 0\n",
            "\n",
            "--- Processing 26/35 ---\n",
            "Scraping: krishna7777\n",
            "Result: Success (Official API)\n",
            "  Rating: 974 (Newbie)\n",
            "  Problems: 312\n",
            "\n",
            "--- Processing 27/35 ---\n",
            "Scraping: KritiB\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 5\n",
            "\n",
            "--- Processing 28/35 ---\n",
            "Scraping: Roshan_nama\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 0\n",
            "\n",
            "--- Processing 29/35 ---\n",
            "Scraping: ayushkumar85371\n",
            "Result: Success (Official API)\n",
            "  Rating: 638 (Newbie)\n",
            "  Problems: 9\n",
            "\n",
            "--- Processing 30/35 ---\n",
            "Scraping: yashpratap291\n",
            "Result: Success (Official API)\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 0\n",
            "\n",
            "=== Checkpoint: 30/35 completed ===\n",
            "Success rate so far: 93.3%\n",
            "\n",
            "--- Processing 31/35 ---\n",
            "Scraping: Thanwal\n",
            "Result: Success (Official API)\n",
            "  Rating: 929 (Newbie)\n",
            "  Problems: 12\n",
            "\n",
            "--- Processing 32/35 ---\n",
            "Scraping: abhi021\n",
            "Result: Success (Official API)\n",
            "  Rating: 932 (Newbie)\n",
            "  Problems: 17\n",
            "\n",
            "--- Processing 33/35 ---\n",
            "Scraping: Sy4aug2004\n",
            "Result: Success (Official API)\n",
            "  Rating: 362 (Newbie)\n",
            "  Problems: 1\n",
            "\n",
            "--- Processing 34/35 ---\n",
            "Scraping: abhinavguptaxia9213\n",
            "Result: Success (Official API)\n",
            "  Rating: 1216 (Pupil)\n",
            "  Problems: 236\n",
            "\n",
            "--- Processing 35/35 ---\n",
            "Scraping: 2023uec1031\n",
            "Result: Success (Official API)\n",
            "  Rating: 417 (Newbie)\n",
            "  Problems: 11\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š FINAL CODEFORCES SCRAPING SUMMARY\n",
            "======================================================================\n",
            "Total profiles processed: 35\n",
            "\n",
            "Detailed status breakdown:\n",
            "  Success (Official API): 33\n",
            "  All methods failed: 2\n",
            "\n",
            "âœ… Successfully scraped 33 profiles!\n",
            "\n",
            "ðŸ† Top performers (by current rating):\n",
            "           Username  Current Rating      Title Problems Solved\n",
            "            Padmesh            1433 Specialist             595\n",
            "     Divyansh_Joshi            1225      Pupil             165\n",
            "abhinavguptaxia9213            1216      Pupil             236\n",
            "       VIKAS__SINGH            1201      Pupil              20\n",
            " dhakadgulshankumar            1139     Newbie             302\n",
            "\n",
            "ðŸŽ¯ Title Distribution:\n",
            "  Newbie: 19\n",
            "  Unrated: 10\n",
            "  Pupil: 3\n",
            "  Specialist: 1\n",
            "\n",
            "ðŸ’¾ Comprehensive data saved to: codeforces_comprehensive_data.csv\n",
            "ðŸ“„ PDF report generated: codeforces_report.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3032ccde-ef35-46b0-9e59-de0988a08bba\", \"codeforces_report.pdf\", 6425)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_67de1556-6773-4f38-ac6b-db8b86ceabbc\", \"codeforces_comprehensive_data.csv\", 6013)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ‰ CodeForces scraping completed successfully!\n",
            "The CSV contains all detailed information including registration dates, countries, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab requests pandas beautifulsoup4\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from google.colab import files\n",
        "from collections import defaultdict\n",
        "\n",
        "# Add your CodeChef profile URLs here\n",
        "codechef_profiles = [\n",
        "    \"https://www.codechef.com/users/Divyansh_Joshi\",\n",
        "    \"https://www.codechef.com/users/chunk_fawn_71\",\n",
        "    \"https://www.codechef.com/users/happy_geese_44\",\n",
        "    \"https://www.codechef.com/users/mayank_devs\",\n",
        "    \"https://www.codechef.com/users/buffy_ring_76\",\n",
        "    \"https://www.codechef.com/users/vivas_mules_01\",\n",
        "    \"https://www.codechef.com/users/rudra_singh_07\",\n",
        "    \"https://www.codechef.com/users/gaurav_1810\",\n",
        "    \"https://www.codechef.com/users/vigil_twirl_45\",\n",
        "    \"https://www.codechef.com/users/harshit3458\",\n",
        "    \"https://www.codechef.com/users/tehseen1639\",\n",
        "    \"https://www.codechef.com/users/crowd_zebra_84\",\n",
        "    \"https://www.codechef.com/users/vikassingh_856\",\n",
        "    \"https://www.codechef.com/users/akshay_20_9_e\",\n",
        "    \"https://www.codechef.com/users/able_mount_21\",\n",
        "    \"https://www.codechef.com/users/kavya_1502\",\n",
        "    \"https://www.codechef.com/users/suhaani17\",\n",
        "    \"https://www.codechef.com/users/dikshit_789\",\n",
        "    \"https://www.codechef.com/users/krishnasharma5\",\n",
        "    \"https://www.codechef.com/users/kriti_b\",\n",
        "    \"https://www.codechef.com/users/roshan_nama_12\",\n",
        "    \"https://www.codechef.com/users/gaze_braid_15\",\n",
        "    \"https://www.codechef.com/users/odd_kitten_45\",\n",
        "    \"https://www.codechef.com/users/abhinav_021\",\n",
        "    \"https://www.codechef.com/users/aabhinavvvvv\",\n",
        "    \"https://www.codechef.com/users/sounder_dice\"\n",
        "    # Add your student profiles here\n",
        "    # Example format: \"https://www.codechef.com/users/username\"\n",
        "]\n",
        "\n",
        "class CodeChefScraper:\n",
        "    def __init__(self):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.9',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Referer': 'https://www.codechef.com/',\n",
        "            'Sec-Fetch-Dest': 'document',\n",
        "            'Sec-Fetch-Mode': 'navigate',\n",
        "            'Sec-Fetch-Site': 'same-origin'\n",
        "        })\n",
        "\n",
        "        # CodeChef star ratings\n",
        "        self.star_ratings = {\n",
        "            (0, 1399): (\"1 Star\", \"#666666\"),\n",
        "            (1400, 1599): (\"2 Star\", \"#1e7d22\"),\n",
        "            (1600, 1799): (\"3 Star\", \"#3366cc\"),\n",
        "            (1800, 1999): (\"4 Star\", \"#684273\"),\n",
        "            (2000, 2199): (\"5 Star\", \"#ffbf00\"),\n",
        "            (2200, 2499): (\"6 Star\", \"#ff7f00\"),\n",
        "            (2500, 9999): (\"7 Star\", \"#d0011b\")\n",
        "        }\n",
        "\n",
        "        # Contest types\n",
        "        self.contest_types = [\n",
        "            'Long Challenge', 'Cook-Off', 'Lunchtime', 'Starters',\n",
        "            'Div 1', 'Div 2', 'Div 3', 'Div 4'\n",
        "        ]\n",
        "\n",
        "    def get_star_rating(self, rating):\n",
        "        \"\"\"Get star rating based on numerical rating\"\"\"\n",
        "        if rating is None or rating == 0:\n",
        "            return \"Unrated\"\n",
        "\n",
        "        for (min_rating, max_rating), (stars, color) in self.star_ratings.items():\n",
        "            if min_rating <= rating <= max_rating:\n",
        "                return stars\n",
        "        return \"Unrated\"\n",
        "\n",
        "    def scrape_with_api(self, username):\n",
        "        \"\"\"Try to use CodeChef's internal APIs\"\"\"\n",
        "        try:\n",
        "            # Method 1: Try the ratings API\n",
        "            api_url = f\"https://www.codechef.com/api/rankings/PRACTICE?itemsPerPage=1&order=asc&search={username}&sortBy=global_rank\"\n",
        "            response = self.session.get(api_url, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if data.get('list') and len(data['list']) > 0:\n",
        "                    user_data = data['list'][0]\n",
        "                    return {\n",
        "                        \"Username\": username,\n",
        "                        \"Global Rank\": user_data.get('global_rank', 'N/A'),\n",
        "                        \"Country Rank\": user_data.get('country_rank', 'N/A'),\n",
        "                        \"Rating\": user_data.get('rating', 0),\n",
        "                        \"Stars\": self.get_star_rating(user_data.get('rating', 0)),\n",
        "                        \"Status\": \"Success (API)\"\n",
        "                    }\n",
        "\n",
        "            # Method 2: Try user details API\n",
        "            user_api_url = f\"https://www.codechef.com/api/user/{username}\"\n",
        "            response = self.session.get(user_api_url, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                try:\n",
        "                    data = response.json()\n",
        "                    # Process API response if successful\n",
        "                    return self.parse_api_response(data, username)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"API method failed for {username}: {str(e)}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def parse_api_response(self, data, username):\n",
        "        \"\"\"Parse API response data\"\"\"\n",
        "        try:\n",
        "            if isinstance(data, dict):\n",
        "                return {\n",
        "                    \"Username\": username,\n",
        "                    \"Rating\": data.get('rating', 0),\n",
        "                    \"Stars\": self.get_star_rating(data.get('rating', 0)),\n",
        "                    \"Global Rank\": data.get('global_rank', 'N/A'),\n",
        "                    \"Country Rank\": data.get('country_rank', 'N/A'),\n",
        "                    \"Country\": data.get('country', 'N/A'),\n",
        "                    \"Status\": \"Success (API Parse)\"\n",
        "                }\n",
        "        except:\n",
        "            pass\n",
        "        return None\n",
        "\n",
        "    def scrape_profile_page(self, username, url):\n",
        "        \"\"\"Scrape the main profile page\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=20)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                result = {\n",
        "                    \"Username\": username,\n",
        "                    \"Real Name\": 'N/A',\n",
        "                    \"Country\": 'N/A',\n",
        "                    \"State\": 'N/A',\n",
        "                    \"City\": 'N/A',\n",
        "                    \"Organization\": 'N/A',\n",
        "                    \"Current Rating\": 0,\n",
        "                    \"Highest Rating\": 0,\n",
        "                    \"Stars\": 'Unrated',\n",
        "                    \"Global Rank\": 'N/A',\n",
        "                    \"Country Rank\": 'N/A',\n",
        "                    \"Problems Solved\": 0,\n",
        "                    \"Total Submissions\": 0,\n",
        "                    \"Contests Participated\": 0,\n",
        "                    \"Best Ranking\": 'N/A',\n",
        "                    \"Status\": \"HTML Scraping\"\n",
        "                }\n",
        "\n",
        "                # Extract data from various sections\n",
        "                self.extract_basic_info(soup, result)\n",
        "                self.extract_rating_info(soup, result)\n",
        "                self.extract_statistics(soup, result)\n",
        "                self.extract_contest_info(soup, result)\n",
        "\n",
        "                return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Profile page scraping failed for {username}: {str(e)}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_basic_info(self, soup, result):\n",
        "        \"\"\"Extract basic user information\"\"\"\n",
        "        try:\n",
        "            # Real name\n",
        "            name_elem = soup.find('h1', class_='h2-style')\n",
        "            if name_elem:\n",
        "                result[\"Real Name\"] = name_elem.get_text(strip=True)\n",
        "\n",
        "            # Location information\n",
        "            info_section = soup.find('section', class_='user-details')\n",
        "            if info_section:\n",
        "                # Country\n",
        "                country_elem = info_section.find('span', title='Country')\n",
        "                if country_elem and country_elem.parent:\n",
        "                    result[\"Country\"] = country_elem.parent.get_text(strip=True).replace('Country:', '').strip()\n",
        "\n",
        "                # State/City\n",
        "                location_text = info_section.get_text()\n",
        "                state_match = re.search(r'State[:\\s]+([^\\n\\r]+)', location_text)\n",
        "                if state_match:\n",
        "                    result[\"State\"] = state_match.group(1).strip()\n",
        "\n",
        "                city_match = re.search(r'City[:\\s]+([^\\n\\r]+)', location_text)\n",
        "                if city_match:\n",
        "                    result[\"City\"] = city_match.group(1).strip()\n",
        "\n",
        "                # Organization\n",
        "                org_match = re.search(r'Student at[:\\s]+([^\\n\\r]+)', location_text)\n",
        "                if not org_match:\n",
        "                    org_match = re.search(r'Works at[:\\s]+([^\\n\\r]+)', location_text)\n",
        "                if org_match:\n",
        "                    result[\"Organization\"] = org_match.group(1).strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Basic info extraction failed: {e}\")\n",
        "\n",
        "    def extract_rating_info(self, soup, result):\n",
        "        \"\"\"Extract rating and ranking information\"\"\"\n",
        "        try:\n",
        "            # Current rating from multiple possible locations\n",
        "            rating_containers = [\n",
        "                soup.find('div', class_='rating-number'),\n",
        "                soup.find('span', class_='rating'),\n",
        "                soup.find('div', {'data-rating': True})\n",
        "            ]\n",
        "\n",
        "            for container in rating_containers:\n",
        "                if container:\n",
        "                    rating_text = container.get_text(strip=True)\n",
        "                    rating_match = re.search(r'(\\d{4})', rating_text)\n",
        "                    if rating_match:\n",
        "                        rating = int(rating_match.group(1))\n",
        "                        if 1000 <= rating <= 4000:  # Valid CodeChef rating range\n",
        "                            result[\"Current Rating\"] = rating\n",
        "                            result[\"Stars\"] = self.get_star_rating(rating)\n",
        "                            break\n",
        "\n",
        "            # Highest rating\n",
        "            highest_elem = soup.find(text=re.compile(r'Highest Rating'))\n",
        "            if highest_elem:\n",
        "                highest_text = highest_elem.parent.get_text() if highest_elem.parent else str(highest_elem)\n",
        "                highest_match = re.search(r'(\\d{4})', highest_text)\n",
        "                if highest_match:\n",
        "                    result[\"Highest Rating\"] = int(highest_match.group(1))\n",
        "\n",
        "            # Global rank\n",
        "            rank_elem = soup.find(text=re.compile(r'Global Rank'))\n",
        "            if rank_elem:\n",
        "                rank_text = rank_elem.parent.get_text() if rank_elem.parent else str(rank_elem)\n",
        "                rank_match = re.search(r'(\\d+)', rank_text.replace(',', ''))\n",
        "                if rank_match:\n",
        "                    result[\"Global Rank\"] = int(rank_match.group(1))\n",
        "\n",
        "            # Country rank\n",
        "            country_rank_elem = soup.find(text=re.compile(r'Country Rank'))\n",
        "            if country_rank_elem:\n",
        "                rank_text = country_rank_elem.parent.get_text() if country_rank_elem.parent else str(country_rank_elem)\n",
        "                rank_match = re.search(r'(\\d+)', rank_text.replace(',', ''))\n",
        "                if rank_match:\n",
        "                    result[\"Country Rank\"] = int(rank_match.group(1))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Rating extraction failed: {e}\")\n",
        "\n",
        "    def extract_statistics(self, soup, result):\n",
        "        \"\"\"Extract problem solving statistics\"\"\"\n",
        "        try:\n",
        "            # Problems solved\n",
        "            solved_elem = soup.find(text=re.compile(r'Problems Solved'))\n",
        "            if solved_elem:\n",
        "                solved_text = solved_elem.parent.get_text() if solved_elem.parent else str(solved_elem)\n",
        "                solved_match = re.search(r'(\\d+)', solved_text)\n",
        "                if solved_match:\n",
        "                    result[\"Problems Solved\"] = int(solved_match.group(1))\n",
        "\n",
        "            # Look for problem count in other locations\n",
        "            if result[\"Problems Solved\"] == 0:\n",
        "                # Try finding in stats section\n",
        "                stats_section = soup.find('div', class_='problems-solved')\n",
        "                if stats_section:\n",
        "                    numbers = re.findall(r'\\d+', stats_section.get_text())\n",
        "                    if numbers:\n",
        "                        result[\"Problems Solved\"] = int(numbers[0])\n",
        "\n",
        "            # Total submissions (harder to find, may need contest history)\n",
        "            submission_elem = soup.find(text=re.compile(r'Submissions'))\n",
        "            if submission_elem:\n",
        "                submission_text = submission_elem.parent.get_text() if submission_elem.parent else str(submission_elem)\n",
        "                submission_match = re.search(r'(\\d+)', submission_text)\n",
        "                if submission_match:\n",
        "                    result[\"Total Submissions\"] = int(submission_match.group(1))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Statistics extraction failed: {e}\")\n",
        "\n",
        "    def extract_contest_info(self, soup, result):\n",
        "        \"\"\"Extract contest participation information\"\"\"\n",
        "        try:\n",
        "            # Contest participation count\n",
        "            contest_elem = soup.find(text=re.compile(r'Contest.*Participated'))\n",
        "            if contest_elem:\n",
        "                contest_text = contest_elem.parent.get_text() if contest_elem.parent else str(contest_elem)\n",
        "                contest_match = re.search(r'(\\d+)', contest_text)\n",
        "                if contest_match:\n",
        "                    result[\"Contests Participated\"] = int(contest_match.group(1))\n",
        "\n",
        "            # Best ranking\n",
        "            best_rank_elem = soup.find(text=re.compile(r'Best Ranking'))\n",
        "            if best_rank_elem:\n",
        "                rank_text = best_rank_elem.parent.get_text() if best_rank_elem.parent else str(best_rank_elem)\n",
        "                rank_match = re.search(r'(\\d+)', rank_text)\n",
        "                if rank_match:\n",
        "                    result[\"Best Ranking\"] = int(rank_match.group(1))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Contest info extraction failed: {e}\")\n",
        "\n",
        "    def scrape_contest_history(self, username):\n",
        "        \"\"\"Scrape contest history for additional stats\"\"\"\n",
        "        try:\n",
        "            contests_url = f\"https://www.codechef.com/users/{username}#contests\"\n",
        "            response = self.session.get(contests_url, timeout=15)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                # Look for contest table or list\n",
        "                contest_rows = soup.find_all('tr') + soup.find_all('div', class_='contest-participation-table')\n",
        "\n",
        "                contest_count = 0\n",
        "                best_rank = float('inf')\n",
        "\n",
        "                for row in contest_rows:\n",
        "                    row_text = row.get_text()\n",
        "                    # Look for ranking numbers\n",
        "                    rank_matches = re.findall(r'\\b(\\d{1,4})\\b', row_text)\n",
        "                    for match in rank_matches:\n",
        "                        rank = int(match)\n",
        "                        if 1 <= rank <= 10000:  # Reasonable contest rank range\n",
        "                            contest_count += 1\n",
        "                            best_rank = min(best_rank, rank)\n",
        "\n",
        "                return {\n",
        "                    'contest_count': contest_count,\n",
        "                    'best_rank': best_rank if best_rank != float('inf') else None\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Contest history scraping failed for {username}: {e}\")\n",
        "\n",
        "        return {'contest_count': 0, 'best_rank': None}\n",
        "\n",
        "    def scrape_profile(self, url):\n",
        "        \"\"\"Main scraping method\"\"\"\n",
        "        username = url.strip(\"/\").split(\"/\")[-1]\n",
        "        print(f\"Scraping: {username}\")\n",
        "\n",
        "        # Method 1: Try API approach\n",
        "        result = self.scrape_with_api(username)\n",
        "        if result and result.get(\"Rating\", 0) > 0:\n",
        "            return result\n",
        "\n",
        "        # Method 2: HTML scraping (main approach)\n",
        "        result = self.scrape_profile_page(username, url)\n",
        "        if result and (result.get(\"Current Rating\", 0) > 0 or result.get(\"Problems Solved\", 0) > 0):\n",
        "            # Enhance with contest history\n",
        "            contest_data = self.scrape_contest_history(username)\n",
        "            if contest_data['contest_count'] > 0:\n",
        "                result[\"Contests Participated\"] = max(result.get(\"Contests Participated\", 0), contest_data['contest_count'])\n",
        "            if contest_data['best_rank']:\n",
        "                result[\"Best Ranking\"] = contest_data['best_rank']\n",
        "            return result\n",
        "\n",
        "        # If all methods fail\n",
        "        return {\n",
        "            \"Username\": username,\n",
        "            \"Real Name\": \"Failed\",\n",
        "            \"Country\": \"Failed\",\n",
        "            \"Current Rating\": \"Failed\",\n",
        "            \"Highest Rating\": \"Failed\",\n",
        "            \"Stars\": \"Failed\",\n",
        "            \"Global Rank\": \"Failed\",\n",
        "            \"Problems Solved\": \"Failed\",\n",
        "            \"Contests Participated\": \"Failed\",\n",
        "            \"Status\": \"All methods failed\"\n",
        "        }\n",
        "\n",
        "def scrape_all_profiles(profiles):\n",
        "    \"\"\"Scrape all CodeChef profiles with progress tracking\"\"\"\n",
        "    scraper = CodeChefScraper()\n",
        "    data = []\n",
        "    total = len(profiles)\n",
        "\n",
        "    for i, url in enumerate(profiles, 1):\n",
        "        print(f\"\\n--- Processing {i}/{total} ---\")\n",
        "        result = scraper.scrape_profile(url)\n",
        "        data.append(result)\n",
        "\n",
        "        # Show current result\n",
        "        print(f\"Result: {result['Status']}\")\n",
        "        if result.get(\"Current Rating\") != \"Failed\":\n",
        "            print(f\"  Rating: {result.get('Current Rating', 0)} ({result.get('Stars', 'N/A')})\")\n",
        "            print(f\"  Problems: {result.get('Problems Solved', 0)}\")\n",
        "            print(f\"  Country: {result.get('Country', 'N/A')}\")\n",
        "\n",
        "        # Rate limiting - CodeChef needs more conservative delays\n",
        "        time.sleep(3)  # 3 second delay to avoid being blocked\n",
        "\n",
        "        # Progress checkpoint\n",
        "        if i % 5 == 0:\n",
        "            print(f\"\\n=== Checkpoint: {i}/{total} completed ===\")\n",
        "            successful = sum(1 for r in data if r['Status'] != 'All methods failed')\n",
        "            print(f\"Success rate so far: {successful/i*100:.1f}%\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def export_to_pdf(df, filename=\"codechef_report.pdf\"):\n",
        "    \"\"\"Export DataFrame to PDF with CodeChef styling\"\"\"\n",
        "    doc = SimpleDocTemplate(filename, pagesize=A4)\n",
        "    elements = []\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    # Title\n",
        "    elements.append(Paragraph(\"CodeChef Comprehensive Leaderboard Report\", styles['Title']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    # Summary statistics\n",
        "    total_users = len(df)\n",
        "    successful_scrapes = len(df[df['Status'].str.contains('Success|HTML', na=False)])\n",
        "    failed_scrapes = len(df[df['Status'].str.contains('Failed', na=False)])\n",
        "\n",
        "    # Calculate statistics for successful scrapes\n",
        "    numeric_df = df[df['Status'].str.contains('Success|HTML', na=False)].copy()\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_cols = ['Current Rating', 'Highest Rating', 'Problems Solved', 'Global Rank', 'Contests Participated']\n",
        "    for col in numeric_cols:\n",
        "        if col in numeric_df.columns:\n",
        "            numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce')\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_rating = numeric_df['Current Rating'].mean() if len(numeric_df) > 0 and 'Current Rating' in numeric_df.columns else 0\n",
        "    avg_problems = numeric_df['Problems Solved'].mean() if len(numeric_df) > 0 and 'Problems Solved' in numeric_df.columns else 0\n",
        "    avg_contests = numeric_df['Contests Participated'].mean() if len(numeric_df) > 0 and 'Contests Participated' in numeric_df.columns else 0\n",
        "\n",
        "    # Count by stars\n",
        "    star_counts = numeric_df['Stars'].value_counts().to_dict() if 'Stars' in numeric_df.columns else {}\n",
        "\n",
        "    # Country distribution\n",
        "    country_counts = numeric_df['Country'].value_counts().head(5).to_dict() if 'Country' in numeric_df.columns else {}\n",
        "\n",
        "    summary_text = f\"\"\"\n",
        "    <b>CodeChef Scraping Summary:</b><br/>\n",
        "    Total Users: {total_users}<br/>\n",
        "    Successfully Scraped: {successful_scrapes}<br/>\n",
        "    Failed Scrapes: {failed_scrapes}<br/>\n",
        "    Success Rate: {successful_scrapes/total_users*100:.1f}%<br/><br/>\n",
        "\n",
        "    <b>Performance Statistics:</b><br/>\n",
        "    Average Current Rating: {avg_rating:.0f}<br/>\n",
        "    Average Problems Solved: {avg_problems:.0f}<br/>\n",
        "    Average Contests Participated: {avg_contests:.0f}<br/><br/>\n",
        "\n",
        "    <b>Star Rating Distribution:</b><br/>\n",
        "    \"\"\"\n",
        "\n",
        "    for stars, count in sorted(star_counts.items()):\n",
        "        summary_text += f\"{stars}: {count}<br/>\"\n",
        "\n",
        "    if country_counts:\n",
        "        summary_text += \"<br/><b>Top Countries:</b><br/>\"\n",
        "        for country, count in country_counts.items():\n",
        "            if country != 'N/A':\n",
        "                summary_text += f\"{country}: {count}<br/>\"\n",
        "\n",
        "    elements.append(Paragraph(summary_text, styles['Normal']))\n",
        "    elements.append(Spacer(1, 12))\n",
        "\n",
        "    # Create main data table\n",
        "    display_cols = ['Username', 'Real Name', 'Current Rating', 'Highest Rating', 'Stars',\n",
        "                   'Global Rank', 'Problems Solved', 'Contests Participated', 'Country', 'Organization']\n",
        "\n",
        "    # Filter columns that exist in the dataframe\n",
        "    available_cols = [col for col in display_cols if col in df.columns]\n",
        "    display_df = df[available_cols]\n",
        "\n",
        "    table_data = [display_df.columns.tolist()] + display_df.values.tolist()\n",
        "\n",
        "    table = Table(table_data, repeatRows=1)\n",
        "    table.setStyle(TableStyle([\n",
        "        (\"BACKGROUND\", (0,0), (-1,0), colors.Color(0.4, 0.2, 0.0)),  # CodeChef brown theme\n",
        "        (\"TEXTCOLOR\",(0,0),(-1,0),colors.whitesmoke),\n",
        "        (\"ALIGN\",(0,0),(-1,-1),\"CENTER\"),\n",
        "        (\"GRID\", (0,0), (-1,-1), 0.5, colors.black),\n",
        "        (\"FONTSIZE\", (0,0), (-1,-1), 5),\n",
        "        (\"VALIGN\", (0,0), (-1,-1), \"MIDDLE\"),\n",
        "        # Alternate row colors\n",
        "        (\"BACKGROUND\", (0,1), (-1,-1), colors.lightgrey),\n",
        "    ]))\n",
        "\n",
        "    elements.append(table)\n",
        "    doc.build(elements)\n",
        "\n",
        "# Main execution\n",
        "print(\"ðŸš€ Starting Comprehensive CodeChef Profile Scraping...\")\n",
        "print(\"Using multiple scraping methods for maximum coverage!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Validate URLs\n",
        "print(\"Validating profile URLs...\")\n",
        "valid_profiles = []\n",
        "for url in codechef_profiles:\n",
        "    if 'codechef.com/users/' in url:\n",
        "        valid_profiles.append(url)\n",
        "    else:\n",
        "        print(f\"âš ï¸  Invalid URL format: {url}\")\n",
        "\n",
        "if not valid_profiles:\n",
        "    print(\"âŒ No valid CodeChef profile URLs found!\")\n",
        "    print(\"Please add URLs in the format: https://www.codechef.com/users/username\")\n",
        "else:\n",
        "    print(f\"âœ… Found {len(valid_profiles)} valid profile URLs\")\n",
        "    print(\"â° Note: CodeChef scraping uses 3-second delays to avoid being blocked\")\n",
        "\n",
        "    # Collect all profiles\n",
        "    data = scrape_all_profiles(valid_profiles)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Detailed summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ðŸ“Š FINAL CODECHEF SCRAPING SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total profiles processed: {len(df)}\")\n",
        "\n",
        "    # Status breakdown\n",
        "    if 'Status' in df.columns:\n",
        "        status_counts = df['Status'].value_counts()\n",
        "        print(\"\\nDetailed status breakdown:\")\n",
        "        for status, count in status_counts.items():\n",
        "            print(f\"  {status}: {count}\")\n",
        "\n",
        "    # Success analysis\n",
        "    successful_df = df[df['Status'].str.contains('Success|HTML', na=False)] if 'Status' in df.columns else df\n",
        "\n",
        "    if len(successful_df) > 0:\n",
        "        print(f\"\\nâœ… Successfully scraped {len(successful_df)} profiles!\")\n",
        "\n",
        "        # Top performers by rating\n",
        "        if 'Current Rating' in successful_df.columns:\n",
        "            print(\"\\nðŸ† Top performers (by current rating):\")\n",
        "            try:\n",
        "                numeric_df = successful_df.copy()\n",
        "                numeric_df['Current Rating'] = pd.to_numeric(numeric_df['Current Rating'], errors='coerce')\n",
        "                top_5 = numeric_df.nlargest(5, 'Current Rating')[['Username', 'Current Rating', 'Stars', 'Problems Solved', 'Country']]\n",
        "                print(top_5.to_string(index=False))\n",
        "            except Exception as e:\n",
        "                print(f\"Could not calculate top performers: {e}\")\n",
        "\n",
        "        # Star distribution\n",
        "        if 'Stars' in successful_df.columns:\n",
        "            print(\"\\nâ­ Star Rating Distribution:\")\n",
        "            star_dist = successful_df['Stars'].value_counts()\n",
        "            for stars, count in star_dist.items():\n",
        "                print(f\"  {stars}: {count}\")\n",
        "\n",
        "        # Country distribution\n",
        "        if 'Country' in successful_df.columns:\n",
        "            print(\"\\nðŸŒ Country Distribution:\")\n",
        "            country_dist = successful_df['Country'].value_counts().head(5)\n",
        "            for country, count in country_dist.items():\n",
        "                if country != 'N/A':\n",
        "                    print(f\"  {country}: {count}\")\n",
        "\n",
        "    # Save comprehensive data\n",
        "    df.to_csv('codechef_comprehensive_data.csv', index=False)\n",
        "    print(f\"\\nðŸ’¾ Comprehensive data saved to: codechef_comprehensive_data.csv\")\n",
        "\n",
        "    # Export to PDF\n",
        "    export_to_pdf(df)\n",
        "    print(\"ðŸ“„ PDF report generated: codechef_report.pdf\")\n",
        "\n",
        "    # Download files\n",
        "    files.download(\"codechef_report.pdf\")\n",
        "    files.download(\"codechef_comprehensive_data.csv\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ CodeChef scraping completed!\")\n",
        "    print(\"ðŸ’¡ Tip: If some profiles failed, try running them individually or check if the usernames are correct.\")\n",
        "    print(\"ðŸ“‹ The CSV contains all detailed information including real names, locations, and organizations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GFLZVss8rkrv",
        "outputId": "29f07047-60bb-4855-f082-cc544e53852f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "ðŸš€ Starting Comprehensive CodeChef Profile Scraping...\n",
            "Using multiple scraping methods for maximum coverage!\n",
            "======================================================================\n",
            "Validating profile URLs...\n",
            "âœ… Found 26 valid profile URLs\n",
            "â° Note: CodeChef scraping uses 3-second delays to avoid being blocked\n",
            "\n",
            "--- Processing 1/26 ---\n",
            "Scraping: Divyansh_Joshi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3419546143.py:243: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  highest_elem = soup.find(text=re.compile(r'Highest Rating'))\n",
            "/tmp/ipython-input-3419546143.py:251: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  rank_elem = soup.find(text=re.compile(r'Global Rank'))\n",
            "/tmp/ipython-input-3419546143.py:259: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  country_rank_elem = soup.find(text=re.compile(r'Country Rank'))\n",
            "/tmp/ipython-input-3419546143.py:273: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  solved_elem = soup.find(text=re.compile(r'Problems Solved'))\n",
            "/tmp/ipython-input-3419546143.py:290: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  submission_elem = soup.find(text=re.compile(r'Submissions'))\n",
            "/tmp/ipython-input-3419546143.py:304: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  contest_elem = soup.find(text=re.compile(r'Contest.*Participated'))\n",
            "/tmp/ipython-input-3419546143.py:312: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  best_rank_elem = soup.find(text=re.compile(r'Best Ranking'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: HTML Scraping\n",
            "  Rating: 1064 (1 Star)\n",
            "  Problems: 2\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 2/26 ---\n",
            "Scraping: chunk_fawn_71\n",
            "Result: HTML Scraping\n",
            "  Rating: 1029 (1 Star)\n",
            "  Problems: 6\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 3/26 ---\n",
            "Scraping: happy_geese_44\n",
            "Result: HTML Scraping\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 30\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 4/26 ---\n",
            "Scraping: mayank_devs\n",
            "Result: HTML Scraping\n",
            "  Rating: 1288 (1 Star)\n",
            "  Problems: 33\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 5/26 ---\n",
            "Scraping: buffy_ring_76\n",
            "Result: All methods failed\n",
            "\n",
            "=== Checkpoint: 5/26 completed ===\n",
            "Success rate so far: 80.0%\n",
            "\n",
            "--- Processing 6/26 ---\n",
            "Scraping: vivas_mules_01\n",
            "Result: HTML Scraping\n",
            "  Rating: 1036 (1 Star)\n",
            "  Problems: 6\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 7/26 ---\n",
            "Scraping: rudra_singh_07\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 8/26 ---\n",
            "Scraping: gaurav_1810\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 9/26 ---\n",
            "Scraping: vigil_twirl_45\n",
            "Result: HTML Scraping\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 194\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 10/26 ---\n",
            "Scraping: harshit3458\n",
            "Result: HTML Scraping\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 16\n",
            "  Country: N/A\n",
            "\n",
            "=== Checkpoint: 10/26 completed ===\n",
            "Success rate so far: 70.0%\n",
            "\n",
            "--- Processing 11/26 ---\n",
            "Scraping: tehseen1639\n",
            "Result: HTML Scraping\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 56\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 12/26 ---\n",
            "Scraping: crowd_zebra_84\n",
            "Result: HTML Scraping\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 5\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 13/26 ---\n",
            "Scraping: vikassingh_856\n",
            "Result: HTML Scraping\n",
            "  Rating: 1582 (2 Star)\n",
            "  Problems: 35\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 14/26 ---\n",
            "Scraping: akshay_20_9_e\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 15/26 ---\n",
            "Scraping: able_mount_21\n",
            "Result: All methods failed\n",
            "\n",
            "=== Checkpoint: 15/26 completed ===\n",
            "Success rate so far: 66.7%\n",
            "\n",
            "--- Processing 16/26 ---\n",
            "Scraping: kavya_1502\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 17/26 ---\n",
            "Scraping: suhaani17\n",
            "Result: HTML Scraping\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 7\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 18/26 ---\n",
            "Scraping: dikshit_789\n",
            "Result: HTML Scraping\n",
            "  Rating: 0 (Unrated)\n",
            "  Problems: 5\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 19/26 ---\n",
            "Scraping: krishnasharma5\n",
            "Result: HTML Scraping\n",
            "  Rating: 1487 (2 Star)\n",
            "  Problems: 129\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 20/26 ---\n",
            "Scraping: kriti_b\n",
            "Result: HTML Scraping\n",
            "  Rating: 1281 (1 Star)\n",
            "  Problems: 16\n",
            "  Country: N/A\n",
            "\n",
            "=== Checkpoint: 20/26 completed ===\n",
            "Success rate so far: 70.0%\n",
            "\n",
            "--- Processing 21/26 ---\n",
            "Scraping: roshan_nama_12\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 22/26 ---\n",
            "Scraping: gaze_braid_15\n",
            "Result: All methods failed\n",
            "\n",
            "--- Processing 23/26 ---\n",
            "Scraping: odd_kitten_45\n",
            "Result: HTML Scraping\n",
            "  Rating: 1369 (1 Star)\n",
            "  Problems: 13\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 24/26 ---\n",
            "Scraping: abhinav_021\n",
            "Result: HTML Scraping\n",
            "  Rating: 1486 (2 Star)\n",
            "  Problems: 170\n",
            "  Country: N/A\n",
            "\n",
            "--- Processing 25/26 ---\n",
            "Scraping: aabhinavvvvv\n",
            "Result: HTML Scraping\n",
            "  Rating: 1551 (2 Star)\n",
            "  Problems: 133\n",
            "  Country: N/A\n",
            "\n",
            "=== Checkpoint: 25/26 completed ===\n",
            "Success rate so far: 68.0%\n",
            "\n",
            "--- Processing 26/26 ---\n",
            "Scraping: sounder_dice\n",
            "Result: HTML Scraping\n",
            "  Rating: 1404 (2 Star)\n",
            "  Problems: 17\n",
            "  Country: N/A\n",
            "\n",
            "======================================================================\n",
            "ðŸ“Š FINAL CODECHEF SCRAPING SUMMARY\n",
            "======================================================================\n",
            "Total profiles processed: 26\n",
            "\n",
            "Detailed status breakdown:\n",
            "  HTML Scraping: 18\n",
            "  All methods failed: 8\n",
            "\n",
            "âœ… Successfully scraped 18 profiles!\n",
            "\n",
            "ðŸ† Top performers (by current rating):\n",
            "      Username  Current Rating  Stars Problems Solved Country\n",
            "vikassingh_856            1582 2 Star              35     N/A\n",
            "  aabhinavvvvv            1551 2 Star             133     N/A\n",
            "krishnasharma5            1487 2 Star             129     N/A\n",
            "   abhinav_021            1486 2 Star             170     N/A\n",
            "  sounder_dice            1404 2 Star              17     N/A\n",
            "\n",
            "â­ Star Rating Distribution:\n",
            "  Unrated: 7\n",
            "  1 Star: 6\n",
            "  2 Star: 5\n",
            "\n",
            "ðŸŒ Country Distribution:\n",
            "\n",
            "ðŸ’¾ Comprehensive data saved to: codechef_comprehensive_data.csv\n",
            "ðŸ“„ PDF report generated: codechef_report.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c8bb6da0-b275-44ff-acbd-917be2b719e1\", \"codechef_report.pdf\", 5728)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75cd9472-84df-4e34-b94b-4fefcfc75c37\", \"codechef_comprehensive_data.csv\", 2635)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ‰ CodeChef scraping completed!\n",
            "ðŸ’¡ Tip: If some profiles failed, try running them individually or check if the usernames are correct.\n",
            "ðŸ“‹ The CSV contains all detailed information including real names, locations, and organizations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aIFvc2BnrqvJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}